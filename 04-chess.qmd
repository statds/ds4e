# Simulation-Based Comparison of Elo and Glicko


## Overview and Learning Goals

This chapter describes a complete, reproducible data science project
designed to compare the Elo and Glicko rating systems using simulation.
The project is intentionally scoped so that it is feasible for beginner
researchers (e.g., high school students) while still addressing questions
that are relevant in the research literature.

The goals of the chapter are:

- To understand Elo and Glicko as statistical learning algorithms.
- To design controlled simulations with known ground truth.
- To evaluate rating systems using principled metrics.
- To practice reproducible computational research.

The chapter proceeds as a step-by-step roadmap, with clearly defined
deliverables at each stage.


## Project Structure and Reproducibility

Before writing any modeling code, set up a reproducible project
structure.

Suppose your project name is `chess-ranking`, its directory layout
would enentually looks like:

```
chess-ranking
  README.md
  _quarto.yml
  src/
    elo.py
    glicko.py
    simulator.py
    update_pipeline.py
    evaluation.py
  tests/
    test_elo.py
    test_glicko.py
  results/
  reports/
```

Key principles:

- All randomness must be controlled by explicit random seeds.
- All numerical constants (e.g., Elo K-factor) should be defined in one
  place.
- Running the full pipeline should produce the same results every time.



## Phase 1: Implementing the Rating Systems


This phase focuses on correctness and clarity rather than computational
efficiency or realism. The goal is to implement each rating system in a
transparent and verifiable way.


### Elo Rating System



In the Elo rating system, each player is assigned a real-valued rating.
Consider a match between player A and player B. Let $R_A$ and $R_B$ denote
their ratings before the match.

After the match, player A’s rating is updated according to

$$
R_A' = R_A + K (S_A - E_A),
$$

where:

- $R_A'$ is player A’s updated rating,
- $K > 0$ is a fixed tuning parameter controlling the size of updates,
- $S_A$ is the observed score for player A,
- $E_A$ is the expected score for player A given the two ratings.

The observed score $S_A$ is defined as
- $S_A = 1$ if player A wins,
- $S_A = 0$ if player A loses.
(We ignore ties for now.)


The expected score $E_A$ is defined by

$$
E_A = \frac{1}{1 + 10^{(R_B - R_A)/400}}.
$$

The update for player B is defined analogously, with
$S_B = 1 - S_A$ and $E_B = 1 - E_A$.

Implementation tasks:

- Write a function that computes the expected score $E_A$ given $R_A$ and
  $R_B$.
- Write a function that updates the ratings of two players after a single
  match.
- Use a fixed value of $K$ (for example, $K = 32$) for all players.

Required tests:

- Symmetry: swapping players A and B swaps the updated ratings.
- Zero-sum property: $(R_A' - R_A) + (R_B' - R_B) = 0$.
- Sanity check: when a much higher-rated player defeats a much lower-rated
  player, the rating update is small.

The final Elo implementation should be short, readable, and fully tested.


### Glicko Rating System

The Glicko (1999) system extends Elo by tracking uncertainty in each
player’s rating. Each player $i$ is represented by

- $r_i$: rating (real-valued, on the Elo-like scale),
- $\phi_i$: rating deviation (RD), with $\phi_i \ge 0$.

Interpretation: $\phi_i$ is a standard-deviation-like uncertainty measure;
smaller $\phi_i$ implies higher confidence in $r_i$. 

The system updates ratings in discrete rating periods. Within a rating
period, each player may play $m$ games. Updates are applied once per period.

Notation for a rating period for a focal player $A$:

- Pre-period state: $(r_A, \phi_A)$.
- Opponents indexed by $j = 1, \dots, m$ with pre-period states
  $(r_j, \phi_j)$.
- Outcomes: $s_j \in \{0, 1\}$ is the score of player $A$ against opponent
  $j$ (win $=1$, loss $=0$). (Draws can be added later with $s_j=1/2$.)
- Constant: $q = \ln(10)/400$.

Step 0 (RD inflation for inactivity)

Let $\phi_A^{\ast}$ denote the inflated RD at the start of the rating
period (before using current results). If $t$ rating periods have elapsed
since player $A$ last competed, inflate by

$$
\phi_A^{\ast} = \min\left(\sqrt{\phi_A^2 + c^2 t},\ 350\right),
$$

where $c > 0$ is a system constant and $350$ is the conventional RD cap
(also used as the RD for an unrated player).

Step 1 (Opponent-weighting function)

For each opponent $j$, define the weight

$$
g(\phi_j) = \frac{1}{\sqrt{1 + \frac{3 q^2 \phi_j^2}{\pi^2}}}.
$$

Opponents with large $\phi_j$ (high uncertainty) receive smaller weight.

Step 2 (Expected score per game)

For each opponent $j$, define the expected score for player $A$ as

$$
E_j
= \frac{1}{1 + 10^{-g(\phi_j)(r_A - r_j)/400}}.
$$

Step 3 (Estimated variance term)

Define

$$
d^2
= \left[
q^2 \sum_{j=1}^m g(\phi_j)^2\, E_j(1 - E_j)
\right]^{-1}.
$$

Step 4 (Rating update)

Update player $A$’s rating by

$$
r_A'
= r_A
+ \frac{q}{\frac{1}{(\phi_A^{\ast})^2} + \frac{1}{d^2}}
\sum_{j=1}^m g(\phi_j)\, (s_j - E_j).
$$

Step 5 (RD update)

Update player $A$’s RD by

$$
\phi_A'
= \sqrt{
\left(
\frac{1}{(\phi_A^{\ast})^2} + \frac{1}{d^2}
\right)^{-1}
}.
$$

Implementation scope

- Implement **Glicko (1999)**, not Glicko-2. 
- Use rating-period updates (batch within-period games, then update once).
- Include RD inflation via $\phi^{\ast}$ and cap RD at $350$.
- Use $q = \ln(10)/400$ and the functions $g(\cdot)$, $E_j$, $d^2$ above.
- Choose $c$ using the guidance in the paper: select $c$ so RD grows from a
  “typical” active-player RD to near $350$ over a chosen number of inactive
  rating periods.

Conceptual interpretation (for beginners)

- Elo uses a fixed step size $K$.
- Glicko uses an adaptive step size.
- Larger $\phi_A^{\ast}$ implies larger updates to $r_A$; smaller
  $\phi_A^{\ast}$ implies smaller updates.

Required tests

- RD contraction: holding opponents fixed, $\phi_A' < \phi_A^{\ast}$ when
  $m \ge 1$ (i.e., playing provides information).
- RD inflation: if $t$ increases (more inactivity), then $\phi_A^{\ast}$
  increases until capped at $350$.
- Shrinking updates: for fixed results, $|r_A' - r_A|$ decreases as
  $\phi_A^{\ast}$ decreases.

Interchangeable module requirement

Define a common match record format and a common update interface.

- Inputs: list of games for the rating period with fields
  (player_id, opponent_id, outcome), plus current player states.
- Outputs:
  - Elo: updated ratings.
  - Glicko: updated ratings and updated RDs.

The calling code should not need to know which system is used, aside from
whether RD is tracked.





## Phase 2: Simulating Match Data

Simulation allows us to evaluate rating systems against known truth.

### Latent Skill Model

Assign each player a true (unobserved) skill value:

\[
\theta_i \sim \mathcal{N}(0, 1).
\]

Match outcomes are generated probabilistically:

\[
\Pr(i \text{ beats } j) = \sigma(\theta_i - \theta_j),
\quad
\sigma(x) = \frac{1}{1 + e^{-x}}.
\]

Only the win/loss outcome is observed by the rating system.



### Experimental Design Switches

The simulator should support three independent design features.

#### Cold-Start

- Players enter the system with no prior games.
- Some players play only a small number of early matches.
- Goal: study early-game behavior and convergence speed.

#### Inactivity

- Each player is assigned an activity rate.
- Time gaps between matches are explicitly simulated.
- Goal: study how ratings behave after long periods without games.

#### Matchmaking Bias

- Opponents are not selected uniformly at random.
- Examples:
  - Similar-skill matchmaking.
  - Stratified leagues.
- Goal: study rating bias induced by non-random schedules.

The simulator should output a single match log containing:
- time index
- player IDs
- match outcome
- true skills (for evaluation only)



## Phase 3: Rating Update Pipeline

Using the simulated match log:

1. Initialize all player ratings.
2. Process matches in chronological order.
3. Update ratings using:
   - Elo
   - Glicko
4. Store rating histories over time.

The output should be a panel-style dataset containing, for each player
and time point:
- estimated rating
- rating deviation (for Glicko)
- number of games played

This dataset is the input for all evaluation steps.



## Phase 4: Evaluation Metrics

Evaluation must be defined *before* inspecting results.

### Predictive Accuracy

Use proper scoring rules:

- Log loss:
  \[
  -\frac{1}{N}\sum
  \left[
    y \log \hat{p} + (1-y)\log(1-\hat{p})
  \right].
  \]

- Brier score:
  \[
  \frac{1}{N}\sum (y - \hat{p})^2.
  \]

Predictions should be computed using ratings *before* each match.



### Skill Recovery

Because true skills are known in simulation:

- Compute correlation between estimated ratings and \(\theta\).
- Compute mean absolute error after rescaling ratings to the skill scale.

These metrics quantify how well each system recovers latent skill.



### Stability and Uncertainty

Key diagnostics:

- Rating volatility by player activity level.
- For Glicko: RD as a function of time since last match.

These diagnostics test whether uncertainty behaves as intended.



### Fairness Under Matchmaking Bias

Evaluate error and rank bias by true-skill deciles.

Questions of interest:

- Are weaker players systematically over- or under-rated?
- Does biased scheduling distort rank order?



## Phase 5: Repeated Experiments and Uncertainty

All results should be averaged over multiple simulation runs
with different random seeds.

Recommended outputs:

- Mean performance metrics with standard errors.
- Sensitivity plots across design parameters.
- Summary tables comparing Elo and Glicko.



## Reporting and Interpretation

The final report should include:

- Explicit assumptions and design choices.
- Clear separation between results and interpretation.
- Discussion of limitations (e.g., simplified skill model).
- Reproducibility instructions.

The emphasis should be on *clarity and insight*, not on algorithmic
novelty.



## Conceptual Takeaways

This project illustrates several core ideas in data science:

- Ratings as online statistical estimators.
- Bias–variance tradeoffs in learning systems.
- The role of uncertainty in sequential decision-making.
- How design choices (e.g., matchmaking) affect inference.

By controlling the data-generating process, students can see these ideas
clearly and connect theory to practice.


## Exercises

+ Phase 1
    - Set up a GitHub repository to store this project.
    - Clones it to an appropriate location on your computer.
    - Add a `src` folder, and in it, add a `elo.py` file.
    - In the `elo.py` file, write two functions, one to update the current ranking, and the other to compute the win probability.

