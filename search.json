[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Everyone",
    "section": "",
    "text": "Preface\nData science is everywhere around us—from the maps that guide our commutes, to the movie recommendations we receive, to the health and environmental reports that help shape public policy. Yet for many students, data science feels distant, complicated, or reserved for college courses. This book takes the opposite view. With the right tools and a curious mindset, anyone—especially high school students—can begin doing real data science today.\nThe goal of this book is to show you what data science actually looks like in practice. It focuses not on memorizing formulas, but on learning how to set up your computer correctly, organize your work, use modern tools such as the command line, Git, VS Code, and Quarto, and build reproducible analyses from day one. These skills form the foundation of professional data science, and developing them early will allow you to create insights that matter.\nThroughout the book, you will work with real datasets from topics you already care about: sports, the environment, health, city life, and more. You will learn how to explore the data, ask thoughtful questions, create visualizations, and communicate your findings clearly. Just as importantly, you will learn how to manage your work like a professional data scientist, building good habits that will serve you through college and beyond.\nThis book assumes no prior experience with programming or statistics. What it does assume is curiosity, patience, and a willingness to try. You will write your first command-line commands, make your first Git commits, produce your first Quarto report, and build your first real project—all in Part I. Part II will invite you to discover patterns in real-world datasets through guided examples.\nMy hope is that this book helps you see data science not as something mysterious, but as something you can do, enjoy, and grow with. Every data scientist started where you are now—with a blank screen, an open mind, and a desire to understand the world better.\nWelcome to your journey into data science.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  What Is Data Science?",
    "section": "",
    "text": "1.1 Data Science Is About Asking Questions\nData science is the practice of learning from data. It brings together three pillars—computation, statistics, and domain knowledge—to create insights that help us understand, predict, and improve the world around us. You have already seen data science in action many times without noticing it.\nThis chapter introduces what data science is and what it is not, setting the stage for the hands-on tools you will learn in Part I.\nEvery data science project begins with a question.\nWhy has traffic increased on my school’s street?\nDo certain neighborhoods file more noise complaints?\nWhich players contributed most to a team’s winning season?\nData science helps translate these questions into something we can analyze, measure, and explain.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What Is Data Science?</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-science-uses-tools-not-just-math",
    "href": "01-intro.html#data-science-uses-tools-not-just-math",
    "title": "1  What Is Data Science?",
    "section": "1.2 Data Science Uses Tools, Not Just Math",
    "text": "1.2 Data Science Uses Tools, Not Just Math\nWhile math is important for deeper understanding, data scientists spend much of their time using tools—command line, Git, VS Code, Quarto, and programming languages such as Python or R. These tools help you:\n\nload and clean data\norganize your work\nbuild reproducible workflows\ncreate visualizations\nshare results with others\n\nThis book starts with tools because tools give you the power to actually do data science. Once you know how to work with your computer the way data scientists do, everything else becomes easier.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What Is Data Science?</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-science-is-reproducible",
    "href": "01-intro.html#data-science-is-reproducible",
    "title": "1  What Is Data Science?",
    "section": "1.3 Data Science Is Reproducible",
    "text": "1.3 Data Science Is Reproducible\nA key idea in this book is reproducibility. A reproducible workflow is one where:\n\nyour analysis is neatly organized\nevery step is documented\nresults can be reproduced exactly by you or anyone else\n\nThis is why you will learn Quarto for writing reports and Git for version control. These skills turn your projects into something professional, even as a beginner.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What Is Data Science?</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-science-is-interdisciplinary",
    "href": "01-intro.html#data-science-is-interdisciplinary",
    "title": "1  What Is Data Science?",
    "section": "1.4 Data Science Is Interdisciplinary",
    "text": "1.4 Data Science Is Interdisciplinary\nTo answer real-world questions, you combine:\n\ncomputing — writing code to process data\nstatistics — measuring uncertainty and learning patterns\ndomain knowledge — understanding the context: climate, health, sports, education, city systems, and many others\n\nNone of these pillars works alone. Together they create meaningful insights.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What Is Data Science?</span>"
    ]
  },
  {
    "objectID": "01-intro.html#what-you-will-learn-next",
    "href": "01-intro.html#what-you-will-learn-next",
    "title": "1  What Is Data Science?",
    "section": "1.5 What You Will Learn Next",
    "text": "1.5 What You Will Learn Next\nPart I of this book focuses on building your foundation:\n\nusing the command line\nnavigating your computer\ncreating projects with clean structure\nusing VS Code as your editing home\ntracking your work with Git\nwriting analyses with Quarto\nlearning your first programming language (Python)\n\nBy the end of Part I, you will have built your first complete data science project: a real report, with real data, that you can publish or share.\nPart II takes you through case studies in health, environment, sports, and other areas—helping you discover patterns in real datasets and understand how data science works in practice.\nData science is not something you learn in one day. It is a craft you build over time. This book gives you the tools and the mindset to begin that journey the right way.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What Is Data Science?</span>"
    ]
  },
  {
    "objectID": "02-computer.html",
    "href": "02-computer.html",
    "title": "2  Your Computer as a Tool for Discovery",
    "section": "",
    "text": "2.1 Introduction\nMany people use computers only through apps, clicking buttons designed by someone else. Data science works differently. Instead of staying inside fixed menus, you will learn to give the computer precise instructions so it can help you ask questions, test ideas, and make new discoveries. Thinking of the computer as a programmable machine opens a new way of working: your results become something you can recreate, improve, and share.\nA key step toward this mindset is understanding how a computer organizes information. Every file on your machine lives in a folder, and every folder has a path that tells the computer exactly where it is. Data scientists work directly with these paths because tools such as Python, R, Git, and Quarto all expect you to know where your work lives. When you understand the file system, you can tell your tools exactly which data to use and where to save your results.\nMuch of data science relies on plain-text files. These include data files like .csv, scripts like .py or .R, and documents like .qmd. Plain text is transparent: you can open it anywhere, track changes, and process it automatically. This clarity is the reason modern analysis avoids mixing computation with formatting. In contrast, spreadsheets hide steps inside cells, and slides require manual updates each time your results change. They are useful for quick checks but cannot support serious analysis where every step must be clear.\nThis leads to one of the most important ideas in data science: reproducibility. Your future self and anyone who reads your work should be able to start with your raw data and code and arrive at the same results. Reproducibility protects you from accidental mistakes, forgotten steps, and lost work. It turns your analysis into something reliable, explainable, and extendable. As you progress through this book, everything you do will be built with reproducibility in mind.\nThis chapter sets the foundation for the tools that follow. Once you see your computer as a programmable partner rather than a collection of apps, learning the command line, version control, and reproducible documents becomes natural.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your Computer as a Tool for Discovery</span>"
    ]
  },
  {
    "objectID": "02-computer.html#whats-inside-your-computer",
    "href": "02-computer.html#whats-inside-your-computer",
    "title": "2  Your Computer as a Tool for Discovery",
    "section": "2.2 What’s Inside Your Computer",
    "text": "2.2 What’s Inside Your Computer\nBefore we talk to the computer through the command line, it helps to know what is inside the box. You do not need to become a hardware expert, but a few ideas will make later chapters much easier to understand.\nAt a high level, every computer used for data science has four key parts:\n\nCPU (Central Processing Unit)\nThe CPU is the “brain” of the computer. It follows instructions one step at a time and does the general-purpose work in your programs.\nRAM (Random-Access Memory)\nRAM is the computer’s short-term memory. When you open a dataset in Python or R, it is copied into RAM so the CPU can work with it quickly. If you do not have enough RAM, large projects slow down or crash.\nStorage (SSD or hard drive)\nStorage is the long-term memory. Your files, photos, code, and datasets live here even after you shut down the computer. Solid-state drives (SSD) are faster and more reliable than older spinning hard drives.\nGPU (Graphics Processing Unit)\nThe GPU started as a special chip for drawing graphics and games. Modern data science uses GPUs for huge math problems, such as training deep learning models, because they can do many simple calculations in parallel.\n\nYou can think of the CPU as a student solving math problems, RAM as the open notebook on the desk, storage as the backpack and bookshelf, and the GPU as a team of helpers who can all work on similar problems at the same time.\n\n\n\n\n\n\nNote⭐ Advanced: How Much Is “Enough”?\n\n\n\n\n\nFor small school projects, almost any modern laptop will work. As you move toward bigger datasets and models, more RAM usually helps more than more CPU cores. If you often run out of memory when loading data, adding RAM or moving to a machine with more RAM makes a big difference. GPUs become important mainly for large deep learning or image-heavy projects.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your Computer as a Tool for Discovery</span>"
    ]
  },
  {
    "objectID": "02-computer.html#operating-systems",
    "href": "02-computer.html#operating-systems",
    "title": "2  Your Computer as a Tool for Discovery",
    "section": "2.3 Operating Systems",
    "text": "2.3 Operating Systems\nAn operating system (OS) is the layer of software that connects your programs to the hardware. It controls files, devices, memory, and how programs run. The three main families you will see in data science are Windows, macOS, and Linux.\n\nLinux\nLinux is the standard in professional data science and on almost all servers and cloud machines. It is predictable: nothing major happens behind your back. When you install software or change a setting, it stays that way until you change it again. The command line tools you learn in this book behave the same way on almost every Linux system.\nmacOS\nmacOS is based on Unix, like Linux, and includes a built-in terminal that understands almost all the same commands. Many data scientists who use laptops choose macOS because it balances a friendly interface with powerful command line tools.\nWindows\nWindows makes it easy to start, but it also tries to “help” by hiding file extensions, changing paths, and running background tools that can confuse reproducible work. Different parts of Windows sometimes disagree about how to name files or run commands. These shortcuts can be convenient for everyday use but can teach habits that do not transfer well to Linux or the cloud.\n\nIn this book, we will prefer a Unix-style command line everywhere. On macOS and Linux, that means using the built-in Terminal app. On Windows, that means installing and using Git Bash, which brings a Unix-like terminal to Windows so that the same commands work on all three platforms.\n\n2.3.1 Installing Software from the Command Line\nData scientists often install tools using package managers, which are programs that download, install, and update software for you from the command line. This is faster, more repeatable, and easier to document than clicking through many installer windows.\nHere are the most common options:\n\nLinux\nEach Linux distribution comes with its own package manager. For example: apt on Ubuntu, dnf on Fedora, and pacman on Arch. These tools let you install almost everything you need with a single command.\nmacOS\nOn macOS, the most popular package manager is Homebrew. After installing Homebrew once, you can run commands like brew install git or brew install python to get new tools.\nWindows\nOn Windows 10 and 11, you can use winget from the command line to install software in Windows Shells (PowerShell or CMD). For example, yo can install Git Bash with winget install Git.Git. Git Bash is a Windows terminal that lets you use the same command-line tools and Git commands commonly used on Linux and macOS. However, winget is a Windows-only tool and should be run in PowerShell or Command Prompt, not in Git Bash.\n\n\n\n\n\n\n\nNote⭐ Advanced: Other Options on Windows\n\n\n\n\n\nSome developers use full Linux environments on Windows through the Windows Subsystem for Linux (WSL) or alternative package managers such as Chocolatey. These are powerful options when you do a lot of development on Windows, but you do not need them for this book.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your Computer as a Tool for Discovery</span>"
    ]
  },
  {
    "objectID": "02-computer.html#how-computers-represent-numbers",
    "href": "02-computer.html#how-computers-represent-numbers",
    "title": "2  Your Computer as a Tool for Discovery",
    "section": "2.4 How Computers Represent Numbers",
    "text": "2.4 How Computers Represent Numbers\nComputers are built on binary, a number system that uses only zeros and ones. That design works well for storing whole numbers, but it creates some surprises when we work with decimals.\nThere are two basic kinds of numbers you will see in data science:\n\nIntegers (…, -2, -1, 0, 1, 2, …)\nThese are whole numbers with no decimal part. Computers can store many integers exactly.\nFloating-point numbers (like 0.1, 2.75, or -3.14)\nThese are used for decimals and measurements. Most real-valued data in science and statistics are stored as floating-point numbers.\n\nBecause computers use binary, many simple-looking decimals cannot be stored exactly. For example, the decimal number 0.1 turns into a long repeating pattern in binary. The computer stores a very close approximation instead of the exact value. When you combine many such numbers, the tiny differences can show up as small rounding errors.\nYou may have seen examples where a language reports that 0.1 + 0.2 is 0.30000000000000004 instead of exactly 0.3. This is not a bug in Python or R. It is a consequence of how floating-point numbers are stored in hardware. Data scientists work with this by rounding results for display and by avoiding direct equality checks with decimals.\nThe key ideas to remember are:\n\nSome decimals cannot be represented exactly on a computer.\nSmall rounding differences are normal in real-number calculations.\nWe usually care about being “close enough” rather than perfectly exact.\n\n\n\n\n\n\n\nNote⭐ Advanced: Why This Matters in Data Science\n\n\n\n\n\nThe standard format for floating-point numbers in most languages is called IEEE 754. It trades exactness for speed and a wide range of values. When you compare floating-point results across languages or machines, tiny differences are expected. When results must be exactly reproducible bit-for-bit, experts sometimes use special libraries, exact arithmetic, or careful control of the hardware and compiler settings.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your Computer as a Tool for Discovery</span>"
    ]
  },
  {
    "objectID": "02-computer.html#the-command-line-speaking-your-computers-language",
    "href": "02-computer.html#the-command-line-speaking-your-computers-language",
    "title": "2  Your Computer as a Tool for Discovery",
    "section": "2.5 The Command Line: Speaking Your Computer’s Language",
    "text": "2.5 The Command Line: Speaking Your Computer’s Language\nMost people use computers through windows, buttons, and icons. The command line offers a different approach: you type short instructions that the computer understands directly. This way of working gives you transparency, repeatability, and control that clicking cannot provide. Data science relies on these qualities because your work must be clear, sharable, and reproducible. On Windows, you will use Git Bash; on macOS or Linux, you will use the Terminal app.\n\n2.5.1 Instructing Your Computer through Command Line\nThe command line is an interface where you communicate with the computer by typing commands. Each command performs one well-defined action. Because every action appears plainly on the screen, the command line makes your steps visible and traceable. This transparency helps you understand what you are doing, and it allows others to follow your work. In contrast, pointing and clicking through mouses leaves no reliable record. The command line also supports automation: a command that you type once can be saved in a script and reused whenever you need it. This repeatability is a cornerstone of reproducible data science.\nWhen you write commands in a script and save them in a file, the computer can repeat those steps exactly. If you make a mistake, you can correct the script and run it again. If a friend wants to understand your analysis, you can send them the script instead of trying to describe what you clicked. This practice of scripting is at the heart of reproducible data science.\nThe command line also gives you access to tools that do not have a graphical interface at all. Many powerful utilities, including Git for version control and Quarto for reproducible documents, are designed to be run from the terminal. Learning the command line opens the door to these tools and lets you combine them in flexible ways.\nAt first, the command line may feel slower than clicking. That feeling fades as you learn the basic commands. Eventually, you will be able to move through folders, manage files, and run complex workflows with just a few keystrokes. Small scripts you write today can become the building blocks for larger projects later.\n\n\n2.5.2 Navigating Your System\nWhen you open Git Bash or Terminal, you start in a current working directory — the folder where the computer assumes you want to work. The commands you type will use this folder unless you tell them otherwise. To use the command line effectively, you need to know where you are and how to move around.\nThe key commands for navigation are:\n\npwd — print working directory (shows your current folder)\n\nls — list files and folders in the current directory\n\ncd &lt;path&gt; — change directory to the folder given by &lt;path&gt;\n\nPaths come in two flavors:\n\nAbsolute paths start from the root of the file system and show the full route to a folder.\n\nRelative paths start from your current location and describe how to get somewhere from there.\n\nFor example, on a typical system your home directory might be something like /Users/alex on macOS or Linux, or C:\\Users\\alex on Windows. In Git Bash, the Windows path will appear in a Unix-style form such as /c/Users/alex. If you are in your home directory and you want to move to a subfolder called projects, you can run cd projects. If you are somewhere else and want to jump straight to your home directory, you can use cd ~.\nThe .. symbol means “the parent directory” — the folder that contains the one you are in:\n\ncd .. moves you up one level.\n\ncd ../.. moves you up two levels.\n\nThe . symbol means “the current directory”. You will often see it when running programs that should use the current folder as their starting point.\nAs you practice, pay attention to the prompt in your terminal. It often shows your current directory or at least the last part of the path. This small detail helps you keep track of where you are working.\n\n\n2.5.3 Managing Files and Folders\nThe command line also lets you create, move, and delete files and folders. At first, these actions may feel risky, but they quickly become a precise way to organize your projects.\nCommon commands include:\n\nmkdir &lt;name&gt; — create a new folder\n\ntouch &lt;filename&gt; — create an empty file\n\nrm &lt;filename&gt; — remove a file\n\nrm -r &lt;folder&gt; — remove a folder and everything inside\n\ncp &lt;source&gt; &lt;destination&gt; — copy a file\n\nmv &lt;old&gt; &lt;new&gt; — rename or move a file\n\nClear and consistent naming makes your work easier to understand and avoids errors later, especially when your projects grow. Good naming practices for data science include:\n\nUse lower case whenever possible (data/, not Data/).\n\nAvoid spaces, which cause trouble in the terminal (raw_data, not raw data).\n\nUse dashes or underscores, but pick one and stay consistent (weather-data or weather_data).\n\nAvoid special characters such as !, ?, *, #, or &.\n\nPrefer short, descriptive names (scripts/, figures/, clean.py).\n\nOrganize by purpose, not by date alone. Use folders like data/, scripts/, projects/, and output/.\n\nKeep related files together, and avoid scattering pieces of the same project across unrelated locations.\n\nGood naming makes your work predictable—for you, your future self, and anyone you collaborate with. It also reduces mistakes when writing paths or running scripts from the terminal.\n\n\n2.5.4 Running Programs from the Terminal\nThe command line can also start programs and check whether your tools are installed correctly. Examples include:\n\ncode . — open the current folder in VS Code\n\ngit --version — check that Git is installed\n\npython --version — check your Python installation\n\nR --version — confirm that R is available\n\nquarto check — verify that Quarto is installed correctly\n\nLaunching programs from the terminal reinforces the idea that your computer is programmable. It also prepares you for workflows where scripts and tools need to run together smoothly.\n\n\n2.5.5 Mini-Project: Creating Your First Data Science Workspace\nTo make these ideas concrete, you will now create a simple workspace for your future projects.\n\nOpen the Terminal\nOn Windows, start Git Bash. On macOS or Linux, open the Terminal app.\nFind Your Home Directory\nRun pwd to display your current working directory.\nIf you are not already in your home directory, move there using cd ~.\nMake a ds4hs Folder\nCreate a directory named ds4hs with mkdir ds4hs.\nMove into it using cd ds4hs.\nConfirm that you are in the right place by running pwd.\nAdd a Few Subfolders\nInside ds4hs, create directories named data, analysis, and figures.\nCheck your work with ls to ensure they appear.\nPractice Moving Around\nChange into the data directory with cd data.\nMove back to ds4hs with cd ...\nTry moving directly to figures using cd figures.\nMove up one level with cd ...\nReturn to the previous directory using cd -.\nOpen a Project Folder in VS Code\nFrom inside the ds4hs directory, open the project in VS Code by running\ncode .\n(If necessary, enable the “code” command in VS Code.)\nCreate a Small Project Template\nInside ds4hs, create a folder named mini_project.\nWithin it, create subfolders data, analysis, and figures.\nAdd an empty Quarto file using touch analysis/analysis.qmd\nand an empty CSV file using touch data/example.csv.\nUse ls -R to display the full directory structure.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your Computer as a Tool for Discovery</span>"
    ]
  },
  {
    "objectID": "02-computer.html#exercises",
    "href": "02-computer.html#exercises",
    "title": "2  Your Computer as a Tool for Discovery",
    "section": "2.6 Exercises",
    "text": "2.6 Exercises\n\nCheck Your Location\nOpen Git Bash or Terminal and run pwd.\nWrite down the full path and circle the folder you are currently in.\nList Folder Contents\nUse ls to display the files and folders in your working directory.\nThen run ls -l and note at least two new pieces of information you see compared with ls.\nCreate a Project Directory\nNavigate to your home directory using cd ~.\nCreate a folder named ds4hs with mkdir ds4hs.\nMove into it with cd ds4hs and verify your location using pwd.\nBuild a Basic Folder Structure\nInside ds4hs, create subfolders for data, analysis, and figures.\nUse ls to confirm that the folders were created.\nPractice moving between them with cd, cd .., and cd -.\nOpen a Project Folder in VS Code\nFrom inside the ds4hs directory, open the project in VS Code by running\ncode .\n(If necessary, enable the code command in VS Code.)\nIn the VS Code terminal, run ls -R to display the full directory structure.\n\n⭐ Challenge (optional):\nInside ds4hs, create a folder named mini_project with subfolders data, analysis, and figures. Add an empty Quarto file using touch analysis/analysis.qmd and an empty CSV file using touch data/example.csv. Use ls -R mini_project to check that everything is in the right place.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your Computer as a Tool for Discovery</span>"
    ]
  },
  {
    "objectID": "03-tools.html",
    "href": "03-tools.html",
    "title": "3  Right Tools for Data Science Projects",
    "section": "",
    "text": "3.1 Introduction\nThis chapter introduces the tools required to carry out reproducible data science projects in this course. A reproducible project is one in which the full analysis — data inputs, code, narrative, and results — can be rerun by someone else, or by the same analyst at a later time, with the same outcomes. Reproducibility is therefore a property of an entire project rather than of any single tool. The goal of this chapter is not to teach tools in isolation, but to explain how a small set of tools work together to support this project-level reproducibility.\nWithout a structured workflow, data science work tends to become fragile. Files are renamed or overwritten, intermediate results are lost, and analysis steps are no longer clear even to the original author. These problems become more severe as projects grow in size or involve collaboration. In an academic setting, they also make it difficult to review, grade, or extend work. The tools introduced in this chapter are motivated by these common failure modes and are chosen to prevent them in a systematic way.\nThe tools in this chapter form a coherent workflow rather than a loose collection of software. Git records the history of a project and makes changes explicit. GitHub provides a shared location for storing, submitting, and reviewing projects. Quarto allows code, text, and results to live in a single executable document. Python, R, or Julia provides the computational engine for data analysis. VS Code serves as a coordinating environment that brings these tools together in daily work. Each tool plays a distinct role, and reproducibility emerges from their interaction.\nThis chapter includes parallel setup sections for Python, R, and Julia. You only need to choose one language and follow that section. If you already know one of these languages, start there. Otherwise, Python, R, and Julia are all reasonable choices. Quarto can technically execute code cells written in different languages within the same document, but for clarity and consistency you are expected to work primarily in a single language. Once your chosen language is set up, you can skip the other language sections; the remainder of the book assumes a single-language workflow rather than switching between languages.\nAll work produced using this setup is treated as a reproducible project. Files are organized in a clear directory structure, tracked with version control, and rendered into executable documents that combine narrative, code, and results. Screenshots, manually edited outputs, and undocumented analysis steps are not substitutes for this workflow. Once learned, this way of working is not limited to data science courses: the same tools can be used to manage assignments in other classes, maintain a research diary, write a blog, draft a novel, or even organize personal notes and creative writing. By the end of this chapter, you should be able to set up a complete project, make changes in a controlled way, and produce work that can be reliably revisited, reused, and shared.\nIn the rest of this chapter, tools are introduced in the order you need them in a real workflow: command line first, then a language, then Quarto, then Git, then an editor, and finally GitHub.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Right Tools for Data Science Projects</span>"
    ]
  },
  {
    "objectID": "03-tools.html#the-command-line-is-the-foundation",
    "href": "03-tools.html#the-command-line-is-the-foundation",
    "title": "3  Right Tools for Data Science Projects",
    "section": "3.2 The Command Line Is the Foundation",
    "text": "3.2 The Command Line Is the Foundation\nThis chapter assumes you have already learned the basics of the command line (also called the shell) earlier in the book. The command line is the foundation for the entire tooling workflow, because it is the same on any computer and it works with any editor.\n\n3.2.1 Why the command line still matters\nFor many beginners, the command line feels old-fashioned compared to graphical menus. In practice, it remains central to technical work for several reasons.\n\nSpeed. Typing commands is often faster than navigating menus. Autocompletion means you rarely type full filenames or commands.\nPrecision. A command specifies exactly which program and which files are used. There is no ambiguity about what happened.\nReproducibility. Commands can be written down, copied, and rerun. Mouse clicks cannot be reliably reproduced.\nUniversality. The same commands work in a regular terminal, on a remote server, or inside an editor such as VSCodium.\nProfessional practice. In movies and television, technical experts are almost always shown typing commands rather than clicking icons. This reflects reality: serious technical work rewards tools that favor speed, precision, and repeatability.\n\nThroughout this chapter, you will repeatedly do three things:\n\nNavigate into a project folder\nRun a program (Git, Quarto, Python/R/Julia)\nCheck what happened (files created, versions, error messages)\n\nWhen you install a tool, you should also verify it from the command line. A typical verification pattern is:\ntoolname --version\nFor example:\ngit --version\nquarto --version\npython --version\nR --version\njulia --version\nYou do not need to memorize commands. The goal is to build the habit of checking what is installed and what version you are using.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Right Tools for Data Science Projects</span>"
    ]
  },
  {
    "objectID": "03-tools.html#the-workflow-before-the-tools-a-conceptual-preview",
    "href": "03-tools.html#the-workflow-before-the-tools-a-conceptual-preview",
    "title": "3  Right Tools for Data Science Projects",
    "section": "3.3 The Workflow Before the Tools (A Conceptual Preview)",
    "text": "3.3 The Workflow Before the Tools (A Conceptual Preview)\nBefore installing anything, it helps to see the full workflow at a high level. Reproducible data science is not one tool. It is a small system of tools that work together.\nA typical workflow looks like this:\n\nWrite a Quarto file (.qmd) that contains text and code\nRun the code using your chosen language (Python, R, or Julia)\nRender the Quarto file into an output document (usually HTML)\nUse Git to record snapshots of your work (commits)\nUse GitHub to share the work and collaborate\n\nThe transcript below is a preview. Do not try to run it yet. You will install the tools and run these commands later, step by step.\n# create a project folder\nmkdir my-project\ncd my-project\n\n# write a Quarto file (in an editor)\n# then render it\nquarto render analysis/report.qmd\n\n# record your work with Git\ngit init\nmkdir analysis\nnotepad analysis/report.qmd\n\ngit add analysis/report.qmd\ngit commit -m \"First report\"\n\n# share your work on GitHub (after setup)\ngit remote add origin &lt;url&gt;\ngit push -u origin main\nIf you understand the purpose of each line in this transcript by the end of the chapter, you have learned the core tooling workflow.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Right Tools for Data Science Projects</span>"
    ]
  },
  {
    "objectID": "03-tools.html#gate-1-choose-a-programming-language-and-set-up-its-environment",
    "href": "03-tools.html#gate-1-choose-a-programming-language-and-set-up-its-environment",
    "title": "3  Right Tools for Data Science Projects",
    "section": "3.4 Gate 1: Choose a Programming Language and Set Up Its Environment",
    "text": "3.4 Gate 1: Choose a Programming Language and Set Up Its Environment\nIn this book, you will write code in one programming language. Choose exactly one language to start (Python, R, or Julia), and follow the installation instructions for that language only. You can always learn a second language later.\nThis gate has one goal: you should be able to run a short piece of code from the command line.\n\n3.4.1 Python\nPython is the most widely used programming language in modern data science and an excellent first language for beginners. Its clean, readable syntax allows students to focus on ideas rather than punctuation, while its large ecosystem means that most common tasks already have well-tested libraries. In this course, Python will be used to write code, analyze data, and produce graphics. These skills form the foundation for reproducible data science workflows introduced later in the book.\nPython has earned its central role because it balances power and accessibility. The base language is expressive and easy to read, and many core tools are included by default. Beyond that, widely used libraries such as pandas for data analysis, matplotlib for visualization, and scikit-learn for machine learning allow students to move quickly from simple examples to realistic projects. While R is also an excellent language, especially for statistics, Python will be the default language in this book, with R treated as an optional companion.\n\n3.4.1.1 Installing Python (Python 3.12)\nWe will use Python 3.12. At the time of writing, this version is supported by major scientific and machine-learning libraries, including TensorFlow and PyTorch. From the beginning, we use the command line to install and manage software so that the workflow is transparent and reproducible.\nChoose the instructions for your operating system.\nWindows (using winget)\nOpen PowerShell (not Git Bash) and run:\nwinget install -e --id Python.Python.3.12\nAfter installation, close and reopen your terminal so that python is available on the PATH.\nmacOS (using Homebrew)\nIf Homebrew is not installed, follow the instructions at https://brew.sh.\nThen install Python 3.12 by running:\nbrew install python@3.12\nHomebrew installs Python in a standard location and makes it available from the terminal.\nLinux (Debian / Ubuntu using apt)\nThese instructions assume a Debian- or Ubuntu-based system.\nFirst update package information:\nsudo apt update\nInstall Python 3.12 and the virtual environment module:\nsudo apt install python3.12 python3.12-venv\nVerify the installation:\npython3.12 --version\nYou should see output similar to:\nPython 3.12.x\nIf this command fails, resolve the installation before continuing.\n\n\n3.4.1.2 Creating a virtual environment\nVirtual environments should be created in a deliberate and consistent location. We recommend creating a dedicated directory in your home folder to store environments for this course.\nFrom a terminal, create a directory named envs in your home directory:\nmkdir -p ~/envs\nCreate a virtual environment named ds-env inside this directory:\npython -m venv ~/envs/ds-env\nActivate the environment:\n\nWindows (Git Bash):\nsource ~/envs/ds-env/Scripts/activate\nmacOS / Linux:\nsource ~/envs/ds-env/bin/activate\n\nWhen the environment is active, your prompt will usually change to show (ds-env).\nUpgrade the package manager and install a small set of core libraries:\npython -m pip install --upgrade pip\npip install numpy pandas matplotlib\nAll Python packages for this course should be installed inside this environment.\nTo leave the environment, run:\ndeactivate\n\n\n3.4.1.3 A short Python warm-up\nBefore moving on, you should be comfortable with:\n\nrunning Python from the command line,\nwriting and executing simple scripts,\nusing variables, lists, and loops,\nimporting and using packages.\n\nFor a concise, official warm-up that can be completed in 2–3 hours, use the following resources from the Python Documentation:\n\nAn Informal Introduction to Python: https://docs.python.org/3/tutorial/introduction.html\nMore Control Flow Tools: https://docs.python.org/3/tutorial/controlflow.html\nModules: https://docs.python.org/3/tutorial/modules.html\n\nRead these sections in order. They provide just enough structure to begin experimenting with Python and to learn additional features on demand.\nAfter completing this warm-up, you should be ready to work interactively and write small scripts. The next section builds on this foundation to introduce a reproducible data science workflow.\n\n\n\n\n\n\n\n\n\n\n\n\n3.4.2 R\nR is a programming language designed for statistical computing and graphics. It is widely used in statistics, biostatistics, and the parts of data science that emphasize modeling, inference, and visualization. R also has an unusually strong culture of packaging and documentation, which makes it well suited for project-based work where others need to rerun and review an analysis.\nR is often preferred when the workflow depends on statistical modeling, publication-quality graphics, or domain-specific methods that are most mature in the R ecosystem. In practice, many projects use both Python and R, selecting the language based on the task rather than loyalty to a tool. This book uses Python as the default engine, but the workflow in this chapter applies equally well to R.\nTo keep R-based projects reproducible, you must manage two things carefully: the R version and the set of R packages used by the project. The installation steps below install R, and the environment steps show how to record and restore project-specific package versions.\n\n3.4.2.1 Installing R (R 4.5)\nChoose the instructions for your operating system.\nWindows (using winget)\nOpen PowerShell (not Git Bash) and run:\nwinget install -e --id RProject.R\nAfter installation, close and reopen your terminal.\nVerify that R is available:\nR --version\nRscript --version\nIf R is not found, it is usually because the installer did not add R to your PATH. In that case, locate your R installation (often under C:\\\\Program Files\\\\R\\\\) and either run R.exe / Rscript.exe from that folder or add the appropriate bin directory to your PATH.\nmacOS (using Homebrew)\nInstall R:\nbrew install r\nVerify:\nR --version\nRscript --version\nLinux (Debian / Ubuntu using apt)\nFor a straightforward installation, use your system package manager:\nsudo apt update\nsudo apt install r-base r-base-dev\nVerify:\nR --version\nRscript --version\nIf your distribution provides an older R than you need, install from the CRAN-maintained Ubuntu repository and follow the current instructions in the CRAN “Ubuntu Packages For R” guide.\n\n\n3.4.2.2 Creating a project library with renv\nR packages are installed into libraries, which are directories on disk. If you install packages globally, different projects can silently share (and overwrite) the same dependencies. This is convenient at first, but it eventually breaks reproducibility when package versions change.\nFor R projects in this book, use renv to manage a per-project package library. renv creates a project-local library and records dependency versions in a lockfile (renv.lock). Anyone with the same R version can restore the project library from that lockfile.\nFirst, install renv (once per machine).\nR -q -e \"install.packages('renv')\"\nThen, from the root directory of an R project, initialize renv:\nR -q -e \"renv::init()\"\nInstall core packages used in many projects:\nR -q -e \"install.packages(c('ggplot2', 'dplyr', 'readr'))\"\nSnapshot the environment to update renv.lock:\nR -q -e \"renv::snapshot()\"\nOn another machine (or after deleting the project library), restore the environment from the lockfile:\nR -q -e \"renv::restore()\"\nIn a Git repository, commit renv.lock and the renv/ infrastructure files created by renv. Do not commit the installed package binaries in renv/library/ unless you have a specific reason to do so.\n\n\n3.4.2.3 A short R warm-up\nBefore moving on, you should be comfortable with:\n\nstarting R from the command line,\nrunning one-line commands with R -e,\nwriting and running scripts with Rscript,\nusing vectors, lists, and data frames,\nloading packages and reading help pages.\n\nFor an official warm-up that can be completed in a few hours, use the R Core Team manual “An Introduction to R” and work through the sections on objects, data structures, and graphics. As you work, practice using help() and help.search() to find documentation from within R.\nAfter completing this warm-up, you should be ready to write small R scripts and to render Quarto documents using R as the execution engine.\n\n\n\n3.4.3 Julia\nJulia is a programming language designed for numerical and scientific computing. It combines a high-level, expressive syntax with performance that is often comparable to low-level languages such as C or Fortran. This design makes Julia attractive for simulation-heavy workloads, optimization, and research code where clarity and speed are both important.\nJulia is increasingly used in data science and applied statistics when projects involve custom algorithms, large simulations, or performance- critical components that would be cumbersome to write in other high-level languages. It is less ubiquitous than Python or R, but its package ecosystem is mature enough for many modeling, visualization, and data manipulation tasks.\nAs with Python and R, reproducibility in Julia depends on controlling the language version and the exact versions of packages used. Julia was designed with this in mind: its built-in package manager records full dependency graphs for each project, making environment management a first-class feature rather than an add-on.\n\n3.4.3.1 Installing Julia (1.x series)\nChoose the instructions for your operating system.\nWindows (using winget)\nOpen PowerShell (not Git Bash) and run:\nwinget install -e --id JuliaLang.Julia\nAfter installation, close and reopen your terminal.\nVerify:\njulia --version\nmacOS (using Homebrew)\nInstall Julia:\nbrew install julia\nVerify:\njulia --version\nLinux\nDownload the official Linux binary from the Julia website and extract it to a convenient location, such as /opt/julia or your home directory. Then add the bin directory to your PATH.\nVerify:\njulia --version\nIf julia is not found, check that the directory containing the Julia binary is on your PATH.\n\n\n3.4.3.2 Project environments with Julia’s package manager\nJulia uses project environments to isolate dependencies. Each project is associated with a Project.toml file that lists direct dependencies and a Manifest.toml file that records the exact versions of all packages, including transitive dependencies.\nTo create or activate a project environment, navigate to the project root directory and start Julia:\njulia\nAt the Julia prompt, activate a local environment:\n]\nactivate .\nThe closing bracket ] switches Julia into package manager mode. The command activate . tells Julia to use a project environment stored in the current directory.\nAdd commonly used packages:\n]\nadd DataFrames CSV Plots\nThis creates (or updates) Project.toml and Manifest.toml in the project directory. These files fully specify the environment.\nOn another machine, or after cloning the repository, activate the project and instantiate the environment:\n]\nactivate .\ninstantiate\nIn a Git repository, commit both Project.toml and Manifest.toml. These files play the same role as requirements.txt or renv.lock in other languages, but with stricter guarantees about reproducibility.\n\n\n3.4.3.3 A short Julia warm-up\nBefore moving on, you should be comfortable with:\n\nstarting Julia from the command line,\nusing the Julia REPL and its help system,\nactivating and instantiating project environments,\nworking with arrays, dictionaries, and tables,\nloading packages with using and import.\n\nA concise and authoritative starting point is the official Julia manual “Getting Started” and the sections on the REPL, packages, and performance tips. After this warm-up, you should be able to write small Julia scripts and use Julia as an execution engine in Quarto documents when a project benefits from Julia’s performance and numerical strengths.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Right Tools for Data Science Projects</span>"
    ]
  },
  {
    "objectID": "03-tools.html#gate-2-quarto-reproducible-documents-for-real-data-science",
    "href": "03-tools.html#gate-2-quarto-reproducible-documents-for-real-data-science",
    "title": "3  Right Tools for Data Science Projects",
    "section": "3.5 Gate 2: Quarto (Reproducible Documents for Real Data Science)",
    "text": "3.5 Gate 2: Quarto (Reproducible Documents for Real Data Science)\nNow that you can run code in your chosen language, you can use Quarto to combine code, text, and results in one reproducible document.\n\n3.5.1 Why Quarto?\nQuarto is a tool for writing documents that combine text, code, figures, and results in a single, executable source file. Instead of keeping separate word-processor files, exported plots, screenshots, and loose scripts, Quarto keeps analysis and narrative together and regenerates results automatically whenever code or data change. This makes work transparent, reproducible, and easier to review.\nQuarto is widely used in data science for technical reports, notebooks, and presentations where correctness and traceability matter. Compared to traditional notebooks, Quarto emphasizes documents first and interactivity second. Compared to Word or PowerPoint, it prioritizes reproducible computation over manual formatting. These properties make Quarto well suited for teaching and for real data science projects, where reasoning and evidence are more important than appearance.\n\n\n3.5.2 Installation and setup (CLI-first)\nQuarto is a command-line tool. We install and use it from the terminal, independent of any editor.\nWindows (using winget)\nOpen PowerShell and run:\nwinget install --id Posit.Quarto -e\nClose and reopen the terminal after installation.\nmacOS (using Homebrew)\nbrew install quarto\nLinux (Debian / Ubuntu)\nsudo apt update\nsudo apt install quarto\nAfter installation, verify that Quarto is available:\nquarto check\nThis command reports available engines (such as Python) and confirms that Quarto is correctly installed.\nEditor support (optional)\nQuarto works entirely from the command line. If you use VS Code, you may optionally install the Quarto extension for syntax highlighting and convenience features. This is not required for rendering documents.\n\n\n3.5.3 Quarto and Python environments\nQuarto does not manage Python installations or virtual environments. When rendering a document, it uses the Python interpreter available on your PATH.\nFor this course, you must activate your course environment before running Quarto commands.\nsource ~/envs/ds-env/bin/activate\nOn Windows (Git Bash):\nsource ~/envs/ds-env/Scripts/activate\nAfter activation, verify:\npython --version\nQuarto will now execute all Python code chunks using this environment.\n\n\n3.5.4 Anatomy of a Quarto file\nA Quarto document (.qmd) has three main components, executed from top to bottom during rendering.\nYAML header\nThe YAML header appears at the top of the file between --- lines and controls document-level settings such as title and output format.\n---\ntitle: \"My Document\"\nformat: html\n---\nMarkdown body\nThe body contains text written in Markdown: headings, paragraphs, lists, links, and mathematical notation. This is where you explain your analysis and interpret results.\nCode chunks\nCode chunks contain executable code. During rendering, Quarto runs the code and inserts the output directly into the document. In this course, we use Python by default.\nprint(“Hello, Quarto”)\n\n\n3.5.5 First reproducible notebook\nA minimal workflow for your first Quarto document is:\n\ncreate a source file,\nwrite text and code together,\nrender the document from the command line.\n\nCreate a new file named my_first.qmd with the following header:\n---\ntitle: \"my-first-quarto-notebook\"\nformat: html\n---\nAdd a short paragraph:\nThis document demonstrates a simple, reproducible analysis written in\nPython using Quarto.\nBefore running this example, make sure the required plotting library is installed in your active environment:\npip install matplotlib\nInsert a Python code chunk that produces a figure:\nimport matplotlib.pyplot as plt\nplt.plot([1, 2, 3], [1, 4, 9]) plt.title(“A Simple Plot”) plt.show()\nActivate your environment and render:\nsource ~/envs/ds-env/bin/activate\nquarto render my_first.qmd\nThis produces an HTML file that contains the text, code, and generated output.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Right Tools for Data Science Projects</span>"
    ]
  },
  {
    "objectID": "03-tools.html#gate-3-git-local-version-control-for-reproducible-projects",
    "href": "03-tools.html#gate-3-git-local-version-control-for-reproducible-projects",
    "title": "3  Right Tools for Data Science Projects",
    "section": "3.6 Gate 3: Git (Local Version Control for Reproducible Projects)",
    "text": "3.6 Gate 3: Git (Local Version Control for Reproducible Projects)\nGit comes after you can run code and render a document, because Git is most useful when it records real work: your source files and the outputs you choose to keep.\n\n3.6.1 Why Git?\nVersion control is a foundational skill for data science because it treats your work as a living project with a complete history. Git lets you track every change you make, recover from mistakes, and collaborate without overwriting anyone’s work. Unlike saving multiple file versions by hand (e.g., project_final_v12_REAL), Git provides a precise, automatic timeline of your edits. This makes your work reproducible, auditable, and shareable, which are essential habits for scientific computing and data science projects.\nGit is used to manage data science projects as evolving artifacts rather than one-time snapshots. Analyses typically change as data are cleaned, models are revised, and interpretations improve. A Git repository should contain only the files needed to understand, reproduce, and extend a project—nothing more.\n\n\n3.6.2 Installing Git\nGit must be installed before it can be used. Use the method appropriate for your system.\n\nWindows (PowerShell):\n\nwinget install --id Git.Git -e\nThis installs Git together with Git Bash, a terminal environment used throughout this book. Do not run winget inside Git Bash.\n\nmacOS (Homebrew):\n\nbrew install git\n\nDebian / Ubuntu Linux (including WSL):\n\nsudo apt update\nsudo apt install git\nAfter installation, configure Git once so your work is properly attributed:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your@email.com\"\nVerify that Git is installed and configured correctly:\ngit --version\ngit config --list\n\n\n3.6.3 Essential Git Commands\nGit is most effective when you stage files deliberately and keep the repository clean. The following commands form the core of everyday Git usage in data science projects.\n\ngit init initializes a repository in a project folder.\ngit status shows which files have changed.\ngit add &lt;file&gt; stages selected files to be recorded.\ngit commit -m \"message\" saves a snapshot of staged changes.\ngit diff displays differences between versions.\ngit push sends committed changes to a remote repository, such as one hosted on GitHub.\ngit pull retrieves updates from a remote repository, such as one hosted on GitHub.\n\nThese two commands are where Git connects to an online platform. A common example of such a platform is GitHub, which serves as a shared location for backing up work and collaborating on projects. The next section focuses on GitHub itself and how it is used in our workflow.\nAvoid using git add . as a default habit. It often stages generated files, temporary outputs, or other unintended content. Staging files explicitly helps keep repositories small, readable, and reproducible.\n\n\n3.6.4 Basic Workflow\nGit works best when your project is organized as a single folder containing scripts, Quarto files, and documentation. A typical workflow follows a simple cycle: edit files, inspect what changed, stage selected files, and commit them with a short message explaining what was done.\ncd my-project\ngit init\ngit status\ngit add README.md\ngit add analysis.qmd\ngit commit -m \"initial project structure\"\nGit can also connect your local project to a remote repository hosted online. This idea is introduced above when discussing git push and git pull, and it is developed in detail in the next section, which focuses on GitHub as a concrete example of an online hosting platform.\n\n\n3.6.5 Good Practices for Git\nGood Git usage is less about memorizing commands and more about developing clean, repeatable habits. The following practices are especially important for reproducible data science projects:\n\nKeep the repository clean. Track only files that are needed to understand, reproduce, and extend the project. Avoid committing generated files, rendered outputs, large raw datasets, or temporary artifacts unless they are explicitly required.\nStage files deliberately. Add files explicitly rather than staging everything at once. This helps prevent accidental inclusion of unnecessary or generated content.\nCommit small, logical changes. Each commit should represent a coherent step in the project, making the history easier to read and reason about.\nWrite informative commit messages. A short message explaining what changed and why is more valuable than a vague description.\nUse .gitignore consistently. The .gitignore file is the primary mechanism for keeping a repository clean over time. It tells Git which files should never be tracked, such as temporary outputs, cache directories, and system-specific files.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Right Tools for Data Science Projects</span>"
    ]
  },
  {
    "objectID": "03-tools.html#gate-4-vscodium-or-vs-code-an-integrated-development-environment",
    "href": "03-tools.html#gate-4-vscodium-or-vs-code-an-integrated-development-environment",
    "title": "3  Right Tools for Data Science Projects",
    "section": "3.7 Gate 4: VSCodium (or VS Code): An Integrated Development Environment",
    "text": "3.7 Gate 4: VSCodium (or VS Code): An Integrated Development Environment\nBy this point, you can already do everything you need from the command line. An editor/IDE does not replace the command line. It makes common tasks easier by putting editing and a terminal in the same place.\n\n3.7.1 What an IDE Is and Why VSCodium (or VS Code)\nAn Integrated Development Environment (IDE) brings together tools for editing files, running code, and organizing projects in a single place. Unlike simple editors such as Notepad or TextEdit, an IDE understands project structure, highlights syntax, reports errors, and integrates with version control and command-line tools.\nVSCodium serves as a coordinating environment for this workflow.\nIf you already use Visual Studio Code (VS Code), you can continue using it. For the purposes of this book, VSCodium and VS Code behave the same.\n\nUnified editor for text, code, and documents\nIntegrated terminal for Git, Python, R, Julia, and Quarto\nBuilt-in file explorer and project navigation\nExtension system for language and tool support\nNative Git integration for version-controlled work\n\nThese features support a project-based, reproducible workflow rather than scattered scripts and files.\n\n\n3.7.2 Installation\nInstall Visual Studio Code using the method appropriate for your system.\nWindows\nInstall VSCodium using winget:\nwinget install -e --id VSCodium.VSCodium\nmacOS\nDownload from the same website, or install via Homebrew:\nbrew install --cask vscodium\nLinux\nInstall using your distribution’s package manager or the official packages provided on the VS Code website.\nAfter installation, launch VS Code and open the integrated terminal (View — Terminal). Confirm that it can see the tools you installed earlier:\ngit --version\nquarto --version\nAlso verify the language you chose to work with:\npython --version   # if using Python\nR --version        # if using R\njulia --version    # if using Julia\nIf these commands run successfully, VS Code is correctly connected to your existing setup.\n\n\n3.7.3 Extensions\nVS Code functionality is extended through extensions. Install a small, focused set that supports reproducible data science work. Use the Extensions icon in the sidebar or open the command palette (Ctrl/Cmd + Shift + P) and choose Extensions: Install Extensions.\nCore extensions (recommended for everyone)\n\nQuarto — Authoring, previewing, and rendering Quarto documents.\nGitLens — Enhanced Git history, blame, and comparison tools.\n\nLanguage-specific extensions (install the one you need)\n\nPython — Editing, linting, debugging, and notebook support for Python.\nR — Editing and execution support for R scripts.\nJulia — Language support for Julia, including code execution.\n\nThese extensions do not install Python, R, Julia, Git, or Quarto. They only connect VS Code to tools you already installed.\n\n\n3.7.4 Integrated Terminal and Shell Choice\nVS Code includes a built-in terminal so you can run command-line tools inside the editor. This keeps editing and execution in one place. Open the terminal with View — Terminal or Ctrl+` (Control + backtick).\nOn Windows, it is often preferable to use Git Bash so that commands match those used in Unix-like environments. After installing Git for Windows, open the command palette, choose Preferences: Open User Settings (JSON), and add:\n\"terminal.integrated.defaultProfile.windows\": \"Git Bash\"\nRestart the terminal. You can now run commands such as cd, ls, git status, and quarto render directly inside VS Code.\n\n\n3.7.5 Working with Projects in VS Code\nA core habit in VS Code is opening folders rather than individual files. When you open a folder, VS Code treats it as a project: it tracks files, recognizes structure, and enables Git integration.\nUse File — Open Folder and select your project directory (for example, ds4e/). The sidebar then shows your files, Git status, and search tools in one place. The command palette provides fast access to nearly all VS Code features and reduces reliance on menus. VS Code also supports split editors for side-by-side viewing and works well for Markdown and Quarto documents.\n\n\n3.7.6 Good Practices\n\nAlways open a project folder rather than individual files.\nKeep all work in plain text: scripts, notes, and Quarto documents.\nUse the integrated terminal for Git, language, and Quarto commands.\nEnable autosave and consider format-on-save for consistency.\nKeep all work inside your project directory, not on the desktop.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Right Tools for Data Science Projects</span>"
    ]
  },
  {
    "objectID": "03-tools.html#gate-5-github-hosting-and-collaboration-for-git-projects",
    "href": "03-tools.html#gate-5-github-hosting-and-collaboration-for-git-projects",
    "title": "3  Right Tools for Data Science Projects",
    "section": "3.8 Gate 5: GitHub (Hosting and Collaboration for Git Projects)",
    "text": "3.8 Gate 5: GitHub (Hosting and Collaboration for Git Projects)\nThis section assumes you already know the basics of Git on your own computer. GitHub adds sharing and collaboration on top of local Git.\n\n3.8.1 What GitHub Is and Why It Matters\nGitHub is an online platform that hosts Git repositories. Git manages project history on your computer, while GitHub stores a synchronized copy online. GitHub adds four capabilities that local Git alone does not provide.\n\nBackup: a remote copy protects against local hardware failure.\nWork anywhere: the same repository can be used on multiple machines; you can work offline and synchronize later.\nCollaboration: multiple contributors can work safely without overwriting each other.\nReview: changes can be inspected and discussed before they are merged.\n\nOnce a local repository is connected to GitHub, only two commands are needed for routine synchronization.\n\ngit push uploads committed local changes to GitHub.\ngit pull downloads changes from GitHub and integrates them locally.\n\n\n\n3.8.2 Authentication on GitHub (SSH)\nGitHub requires authentication before it accepts git push. A common method is SSH key authentication.\nSSH uses a key pair.\n\nPrivate key: stays on your computer and must never be shared.\nPublic key: added to your GitHub account to identify your computer.\n\nGenerate a key pair locally.\nssh-keygen -t ed25519 -C \"your_email@example.com\"\nThe key pair will be stored in the .ssh folder in your home directory.\nDisplay the public key.\ncat ~/.ssh/id_ed25519.pub\nCopy the public key to your clipboard.\n\nmacOS (Terminal):\n\npbcopy &lt; ~/.ssh/id_ed25519.pub\n\nWindows (Git Bash):\n\ncat ~/.ssh/id_ed25519.pub | clip\n\nWindows (PowerShell):\n\nGet-Content $env:USERPROFILE\\.ssh\\id_ed25519.pub | clip\n\nLinux / WSL:\n\nsudo apt update\nsudo apt install xclip\nxclip -selection clipboard &lt; ~/.ssh/id_ed25519.pub\nAdd the public key on GitHub.\n\nProfile menu — Settings\nSSH and GPG keys — New SSH key\nPaste the key and save\n\nTest the connection.\nssh -T git@github.com\nFor up-to-date authentication, one may ask an AI assistant for help for your operating system. For example:\n\n“How do I set up SSH keys for GitHub on Windows using Git Bash and verify that git push works?”\n\n\n\n3.8.3 Creating and Publishing a New Repository\nThis workflow applies only if you already have a local Git repository. If not, create one first with git init.\nFirst, create a new empty repository on GitHub (do not initialize it with a README). Then connect your local repository to GitHub.\ngit remote add origin git@github.com:yourname/example-repo.git\ngit branch -M main\ngit push -u origin main\nHere, yourname is your GitHub username and example-repo is the repository name you chose on GitHub.\n\n\n3.8.4 Cloning an Existing Repository\nCloning is used when the repository already exists on GitHub. Suppose that someone is the GitHub username or organization that owns a repository called project. To clone this repository to your own computer, first cd to the folder where you want put the repository and then:\ngit clone git@github.com:someone/project.git\nCloning creates a new local directory containing the full project history.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Right Tools for Data Science Projects</span>"
    ]
  },
  {
    "objectID": "03-tools.html#first-data-science-project-putting-it-all-together",
    "href": "03-tools.html#first-data-science-project-putting-it-all-together",
    "title": "3  Right Tools for Data Science Projects",
    "section": "3.9 first-data-science-project: Putting It All Together",
    "text": "3.9 first-data-science-project: Putting It All Together\nThis project section is a guided recap. You will reuse the same tools from the earlier gates, in the same order.\nThis chapter integrates everything from Part I—command line, VS Code, Git, GitHub, Quarto, and Python/R—into a single, coherent project workflow. The goal is to let students experience how real data science work is done: create a clean folder, version it with Git, write a reproducible Quarto file, and share the final analysis.\n\n3.9.1 Choosing a Simple, Meaningful Dataset\nThe first step in any project is choosing a dataset that is small, clean, and intrinsically interesting. Students should select something they care about so they remain motivated while practicing the workflow. Good examples include:\n\nNYC 311 complaint counts for a single neighborhood\nSchool lunch nutrition data from USDA open data\nA small sports dataset (NBA scores, soccer goals, WNBA box scores)\nTrends in daily steps from a personal fitness tracker\nAny two-column CSV they record themselves (date + measurement)\n\nBest practice is to avoid large, messy datasets for this first project. Students should aim to complete end-to-end analysis, not get stuck in heavy cleaning.\n\n\n3.9.2 Setting Up the Project Folder\nA clean folder structure helps keep the project reproducible and organized. Students use the command line to create folders and set up a Git repository.\nRecommended structure:\n\ndata/ — raw datasets in CSV or JSON\nanalysis/ — Quarto notebooks\nfigures/ — automatically generated plots\nREADME.md — short description of the project\n\nKey steps:\n\nUse the command line to create the folder and subfolders\n(mkdir my-project, cd my-project, mkdir data analysis figures)\nInitialize Git with\ngit init\nMake the first commit with\ngit add README.md and git commit -m \"Initial project structure\"\n\nStudents should verify that Git is tracking the project by running git status and confirming the working tree is clean.\n\n\n3.9.3 Writing a Full Quarto Analysis\nThe core of the project is a reproducible Quarto notebook that explains the data, code, and conclusions in one document. The notebook should include:\n\nA clear statement of the question (e.g., “How do 311 noise complaints differ between weekdays and weekends?”)\nCode to import the dataset\nTwo or three meaningful visualizations (bar plots, line plots, scatterplots, histograms)\nShort summary paragraphs explaining the patterns\n\nA minimal workflow:\n\nCreate analysis/project.qmd in VS Code.\nAdd a YAML header with a title, author, and format.\nInsert code chunks to load the dataset and inspect its structure.\nGenerate plots and save outputs to the figures/ folder.\nRender the notebook to HTML using the VS Code Quarto extension.\n\nStudents should keep text and code in one place—not separate PowerPoints, Word files, or screenshots. Quarto ensures everything is reproducible.\n\n\n3.9.4 Publishing or Sharing Work\nOnce the analysis renders cleanly, students can make the project public (or share privately).\nOptions include:\n\nPush the project to GitHub with\ngit add README.md analysis/project.qmd, git commit, and git push\nShare the rendered HTML via a GitHub repository\nOptionally, enable GitHub Pages so the report becomes a public website at https://username.github.io/my-project\n\nThis final step completes the full data science cycle: version control, reproducible notebook, and public sharing.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Right Tools for Data Science Projects</span>"
    ]
  }
]