[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science for Everyone",
    "section": "",
    "text": "Preface\nData science is everywhere around us—from the maps that guide our commutes, to the movie recommendations we receive, to the health and environmental reports that help shape public policy. Yet for many students, data science feels distant, complicated, or reserved for college courses. This book takes the opposite view. With the right tools and a curious mindset, anyone—especially high school students—can begin doing real data science today.\nThe goal of this book is to show you what data science actually looks like in practice. It focuses not on memorizing formulas, but on learning how to set up your computer correctly, organize your work, use modern tools such as the command line, Git, VSCodium, and Quarto, and build reproducible analyses from day one. These skills form the foundation of professional data science, and developing them early will allow you to create insights that matter.\nThroughout the book, you will work with real datasets from topics you already care about: sports, the environment, health, city life, and more. You will learn how to explore the data, ask thoughtful questions, create visualizations, and communicate your findings clearly. Just as importantly, you will learn how to manage your work like a professional data scientist, building good habits that will serve you through college and beyond.\nThis book assumes no prior experience with programming or statistics. What it does assume is curiosity, patience, and a willingness to try. You will write your first command-line commands, make your first Git commits, produce your first Quarto report, and build your first real project—all in Part I. Part II will invite you to discover patterns in real-world datasets through guided examples.\nMy hope is that this book helps you see data science not as something mysterious, but as something you can do, enjoy, and grow with. Every data scientist started where you are now—with a blank screen, an open mind, and a desire to understand the world better.\nWelcome to your journey into data science.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 What Is Data?\nData science is the practice of learning from data. In simple terms, it means using information to answer questions and make better decisions. It brings together three pillars—computation, statistics, and domain knowledge—to discover patterns and evidence that help us understand, predict, and improve the world around us. You have already seen data science in action many times without noticing it.\nThis chapter introduces what data science is and what it is not, setting the stage for the hands-on tools you will learn in Part I.\nData are pieces of recorded information.\nThey can be:\nSome data are structured, meaning they are organized in a clear and consistent format. Often this format is a table with rows and columns, like a spreadsheet or database table. Structured data are easy for computers to sort, filter, and analyze.\nOther data are unstructured, meaning they do not come neatly organized. A long paragraph of text, a photograph, or an audio recording are all examples of unstructured data. They still contain information—but that information may need to be transformed into a more organized format before certain types of analysis are possible.\nIt is important to note that data do not have to fit into rows and columns to be real data. Images, videos, and text can be analyzed directly using specialized tools. However, many common statistical methods work most easily when data are arranged in a structured format.\nTo see this more clearly, imagine a small table about students in a class:\nEach row is an observation (one student).\nEach column is a variable (a recorded characteristic).\nWhen information is organized like this, a computer can sort it, analyze it, and visualize it efficiently.\nHere is that same table created in Python.\nimport pandas as pd\n\n# create a small structured dataset\nstudents = pd.DataFrame({\n    \"Name\": [\"Alex\", \"Jordan\", \"Sam\"],\n    \"Grade\": [88, 92, 75],\n    \"Favorite_Subject\": [\"Math\", \"History\", \"Biology\"]\n})\n\nstudents\n\n\n\n\n\n\n\n\nName\nGrade\nFavorite_Subject\n\n\n\n\n0\nAlex\n88\nMath\n\n\n1\nJordan\n92\nHistory\n\n\n2\nSam\n75\nBiology\nNow the information is structured data that a computer can analyze directly.\nIf instead we had written:\n“Alex scored 88 and likes Math. Jordan scored 92 and likes History…”\nthat would still be data—but it would not be organized in a consistent structure. Before we could apply many standard statistical tools, we would likely reorganize it into a table.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#what-is-data",
    "href": "01-intro.html#what-is-data",
    "title": "1  Introduction",
    "section": "",
    "text": "numbers (test scores, temperatures, distances)\ncategories (favorite food, school subject, city name)\ntext (reviews, comments, tweets)\nimages or sounds\n\n\n\n\n\n\n\n\nName\nGrade\nFavorite_Subject\n\n\n\n\nAlex\n88\nMath\n\n\nJordan\n92\nHistory\n\n\nSam\n75\nBiology\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteCheck-in Question\n\n\n\nCan you think of an example of data that would be difficult to fit into rows and columns?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-science-begins-with-questions",
    "href": "01-intro.html#data-science-begins-with-questions",
    "title": "1  Introduction",
    "section": "1.2 Data Science Begins with Questions",
    "text": "1.2 Data Science Begins with Questions\nEvery data science project begins with a question.\nWhy has traffic increased on my school’s street?\nDo certain neighborhoods file more noise complaints?\nWhich players contributed most to a team’s winning season?\nBut not every question works well for data science. Good data science questions are specific and measurable. That means we can connect them directly to data.\nFor example:\nVague question:\nWhy are students stressed?\nMore precise question:\nHow many hours do students report studying per week, and how does that relate to reported stress levels?\nThe second question can be answered with collected data. The first one is too broad without further definition.\nWe can even simulate small data to make the question concrete.\n\nimport numpy as np\nnp.random.seed(42)\n\nhours_studying = np.random.randint(1, 6, size=10)\nstress_level = np.random.randint(1, 6, size=10)\n\nlist(zip(hours_studying, stress_level))\n\n[(np.int64(4), np.int64(4)),\n (np.int64(5), np.int64(3)),\n (np.int64(3), np.int64(5)),\n (np.int64(5), np.int64(2)),\n (np.int64(5), np.int64(4)),\n (np.int64(2), np.int64(2)),\n (np.int64(3), np.int64(4)),\n (np.int64(3), np.int64(5)),\n (np.int64(3), np.int64(1)),\n (np.int64(5), np.int64(4))]\n\n\nThis simple example generates pairs of numbers: (hours studying, stress level). Now we could measure whether higher study time is associated with higher stress.\nData science helps translate everyday curiosity into clear, answerable questions. Once a question becomes measurable, we can analyze it, test ideas, and draw conclusions based on evidence.\n\n\n\n\n\n\nNoteCheck-in Question\n\n\n\nCan you think of a question about your school that could be rewritten to make it measurable?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-science-uses-tools",
    "href": "01-intro.html#data-science-uses-tools",
    "title": "1  Introduction",
    "section": "1.3 Data Science Uses Tools",
    "text": "1.3 Data Science Uses Tools\nData science is not only about ideas. It is also about building systems that allow you to work carefully and efficiently.\nWhile mathematics and statistics help us reason about data, data scientists spend much of their time using computational tools. These tools allow us to load data, clean it, analyze it, and communicate results in a clear and repeatable way.\nSome of the most important tools you will learn include:\n\nthe command line — a text-based way to control your computer\nGit — a system for tracking changes in your files\nan IDE (Integrated Development Environment) — a program for writing, organizing, and running code\nQuarto — a publishing system that combines code and explanation\na programming language such as Python\n\nEach tool has a specific role.\nThe command line allows you to navigate folders, create files, and run programs without clicking through menus. This makes your work faster and easier to automate.\nGit records every change you make to a project. If you make a mistake, you can return to an earlier version. If you collaborate with others, Git keeps everyone’s work organized.\nAn IDE (Integrated Development Environment) is more than a simple text editor. It allows you to write code, run programs, manage files, and debug errors in one place. Many different IDEs exist, including free and open-source options. The important idea is not the brand of software, but the workflow it supports.\nQuarto allows you to write explanations and code in the same document. When you render the file, Quarto runs the code and inserts the results directly into your report. This connects your reasoning and your computation in one reproducible workflow.\nYou may already be familiar with graphical user interfaces (GUIs), where you click buttons and select options from menus. GUIs are useful for quick exploration and one-time tasks. However, steps performed only by clicking are often difficult to document precisely.\nFor this reason, professional data scientists prefer writing code. Code records every instruction explicitly. That makes analyses easier to verify, repeat, and improve.\nFor example, here is a small calculation written in Python:\n\nnumbers = [3, 7, 10]\nsum(numbers)\n\n20\n\n\nInstead of calculating by hand or clicking through menus, you write the instruction once and let the computer execute it exactly.\nThese tools work together. The command line manages your environment. Git tracks your changes. Your IDE organizes and runs your code. Quarto publishes your results. Python performs the computation.\nLearning these tools gives you the ability to actually do data science, not just talk about it.\n\n\n\n\n\n\nNoteGUI vs. Code\n\n\n\nGraphical user interfaces (GUIs) allow you to click buttons and choose options from menus. They are helpful for quick exploration and small tasks.\nHowever, when you perform steps only by clicking, those steps may not be recorded in a clear, repeatable way. If someone asks, “Exactly what did you do?”, it can be difficult to answer precisely.\nCode works differently. Every instruction is written down. That means the computer — and other people — can rerun the exact same steps.\nIn data science, we prefer tools that make our work transparent and reproducible.\n\n\n\n\n\n\n\n\nNoteCheck-in Question\n\n\n\nWhy might writing code be more reliable than performing calculations by hand or clicking through menus, especially for large datasets?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-science-is-reproducible",
    "href": "01-intro.html#data-science-is-reproducible",
    "title": "1  Introduction",
    "section": "1.4 Data Science Is Reproducible",
    "text": "1.4 Data Science Is Reproducible\nReproducibility is a core principle of data science. It means that someone else, or even you in the future, can run the same analysis and obtain the same results.\nIn professional settings, results must be verifiable. If a report claims that average test scores increased by 5 points, others should be able to examine the data and confirm the calculation. Without reproducibility, findings are difficult to trust.\nImagine finishing a project and coming back to it six months later. Would you remember:\n\nwhere the data came from?\nwhat cleaning steps you applied?\nwhich commands produced your final results?\n\nIf the answer is no, the work is not reproducible.\nA reproducible workflow is one where:\n\nyour files are neatly organized\nevery step is written as code (not done by clicking)\nresults can be generated again automatically\nchanges are tracked over time\n\nFor example, suppose you calculate the average test score.\n\nscores = [88, 92, 75]\nsum(scores) / len(scores)\n\n85.0\n\n\nIf that calculation lives inside your script, anyone can rerun it. But if you computed it on a calculator and only wrote down the final number, no one can verify how you obtained it.\nReproducibility protects you from mistakes. If something looks wrong, you can rerun everything from the beginning. It also builds trust. Others can inspect your code and confirm your results.\nThis is why data scientists rely on tools such as:\n\nthe command line to control their environment\nGit to track changes\nQuarto to combine code and explanation in one document\n\nTogether, these tools allow an entire project to be rebuilt from scratch using documented steps.\n\n\n\n\n\n\nNoteCheck-in Question\n\n\n\nIf a classmate wanted to reproduce your project, what files would they need? Would a screenshot of your results be enough?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#data-science-is-interdisciplinary",
    "href": "01-intro.html#data-science-is-interdisciplinary",
    "title": "1  Introduction",
    "section": "1.5 Data Science Is Interdisciplinary",
    "text": "1.5 Data Science Is Interdisciplinary\nData science is interdisciplinary. That means it combines knowledge from multiple fields rather than relying on just one.\nTo answer real-world questions, data scientists bring together three main pillars:\n\ncomputing — writing code to process and manage data\nstatistics — measuring variation and uncertainty\ndomain knowledge — understanding the real-world context\n\nEach pillar plays a different role.\nComputing allows us to handle large datasets efficiently. Without code, it would be nearly impossible to analyze millions of records.\nStatistics helps us reason carefully about patterns. It teaches us how to distinguish real signals from random variation. Without statistics, we might mistake coincidence for evidence.\nDomain knowledge provides context. Data do not explain themselves. If you are analyzing hospital data, you need basic knowledge of health care systems. If you are studying sports performance, you need to understand the rules of the game.\nConsider an example. Suppose a city wants to predict flooding.\n\nComputing allows us to process rainfall data and elevation maps.\nStatistics helps us estimate the probability of flooding.\nDomain knowledge tells us how drainage systems and local geography influence water flow.\n\nIf any one pillar is missing, the analysis becomes weaker.\nComputing without statistics may produce precise but misleading numbers. Statistics without computing may be too slow to apply at scale. Both without domain knowledge may lead to unrealistic conclusions.\nData science works best when these three areas support each other.\n\n\n\n\n\n\nNoteCheck-in Question\n\n\n\nThink of a problem you care about (sports, music, climate, school policy). What kind of domain knowledge would you need to analyze it properly?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#what-data-science-is-not",
    "href": "01-intro.html#what-data-science-is-not",
    "title": "1  Introduction",
    "section": "1.6 What Data Science Is Not",
    "text": "1.6 What Data Science Is Not\nData science is not a shortcut around mathematics and statistics. When students try to rely only on software without understanding probability, variation, or logical reasoning, they often produce results that look polished but are unreliable. A model can generate precise numbers, yet those numbers may rest on weak assumptions. Strong quantitative training does not make work complicated for its own sake; it prevents false confidence.\nData science is also not an exercise in writing code for its own sake. A well-written program can process enormous amounts of data, but speed does not guarantee truth. If the underlying logic is flawed, computing simply amplifies the mistake. Learning to program well means learning to think carefully about algorithms, structure, and consequences.\nMost importantly, data science is not magic. No method can extract reliable information from pure noise. Humans are naturally drawn to patterns, even when they occur by chance. Without statistical discipline, it is easy to mistake randomness for discovery. History is full of examples where weak evidence led to strong claims, causing confusion or harm. Responsible data science requires skepticism, patience, and respect for limits.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#what-you-will-learn-next",
    "href": "01-intro.html#what-you-will-learn-next",
    "title": "1  Introduction",
    "section": "1.7 What You Will Learn Next",
    "text": "1.7 What You Will Learn Next\nPart I of this book focuses on building your foundation:\n\nusing the command line\nnavigating your computer\ncreating projects with clean structure\nusing VSCodium as your editing home\ntracking your work with Git\nwriting analyses with Quarto\nlearning your first programming language (Python)\n\nBy the end of Part I, you will have built your first complete data science project: a real report, with real data, that you can publish or share.\nPart II takes you through case studies in health, environment, sports, and other areas—helping you discover patterns in real datasets and understand how data science works in practice.\nData science is not something you learn in one day. It is a craft you build over time. This book gives you the tools and the mindset to begin that journey the right way.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "01-intro.html#exercises",
    "href": "01-intro.html#exercises",
    "title": "1  Introduction",
    "section": "1.8 Exercises",
    "text": "1.8 Exercises\nThese questions are designed to help you think more carefully about the ideas in this chapter. Write short answers unless instructed otherwise.\n\nIn your own words, define data science. What are its three main pillars?\nGive one example of structured data and one example of unstructured data from your daily life.\nRewrite the following vague question so that it becomes measurable:\n“Are students happy at our school?”\nExplain why clicking through menus in a GUI might make an analysis harder to reproduce.\nImagine you conducted a small survey of 10 classmates about how many hours they sleep each night.\n\nWhat would be one possible variable?\n\nWhat would be one observation?\n\nWhy is reproducibility important in professional settings? Give one practical reason.\nChoose a real-world topic you care about (sports, music, climate, social media, school policy, etc.).\n\nPropose one clear, measurable data science question.\n\nIdentify what kind of domain knowledge you would need.\n\nOptional CLI Warm-Up.\n\nOpen your terminal.\n\nType:\npwd\nWhat does this command display?\nNow type:\nls\nWhat do you see?\nIn one sentence, explain how using the command line differs from clicking through folders.\n\nMini-Project Preview.\n\nWrite one clear, measurable question you would like to answer.\n\nDescribe what data you would need.\n\nExplain how you would organize those data in a table.\n\nWhy would reproducibility matter for this project?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "02-computer.html",
    "href": "02-computer.html",
    "title": "2  Your Computer as a Tool for Discovery",
    "section": "",
    "text": "2.1 Introduction\nMany people use computers only through apps, clicking buttons designed by someone else. Data science works differently. Instead of staying inside fixed menus, you will learn to give the computer precise instructions so it can help you ask questions, test ideas, and make new discoveries. Thinking of the computer as a programmable machine opens a new way of working: your results become something you can recreate, improve, and share.\nIn Chapter 1, we discussed graphical user interfaces (GUIs). GUIs are convenient because they hide complexity. You click a button and something happens. But when steps are hidden, it becomes harder to explain exactly what was done. In data science, hidden steps create risk. If you cannot describe precisely what the computer did, you cannot fully trust the result.\nThe command line is different. It does nothing unless you tell it to do something. It does not guess your intention. It does not silently fix your mistakes. It waits for instructions. That clarity may feel strict, but it is powerful. You are not at the mercy of the computer. You instruct it.\nA key step toward this mindset is understanding how a computer organizes information. Every file on your machine lives in a folder, and every folder has a path that tells the computer exactly where it is. Data scientists work directly with these paths because tools such as Python, R, Git, and Quarto all expect you to know where your work lives. When you understand the file system, you can tell your tools exactly which data to use and where to save your results.\nMuch of data science relies on plain-text files. These include data files like .csv, scripts like .py or .R, and documents like .qmd. Plain text is transparent: you can open it anywhere, track changes, and process it automatically. This clarity is the reason modern analysis avoids mixing computation with formatting. In contrast, spreadsheets hide steps inside cells, and slides require manual updates each time your results change. They are useful for quick checks but cannot support serious analysis where every step must be clear.\nThis leads to one of the most important ideas in data science: reproducibility. Your future self and anyone who reads your work should be able to start with your raw data and code and arrive at the same results. Reproducibility protects you from accidental mistakes, forgotten steps, and lost work. It turns your analysis into something reliable, explainable, and extendable. As you progress through this book, everything you do will be built with reproducibility in mind.\nLearning to use your computer this way requires an engineering mindset. You must be comfortable giving clear instructions, checking results, and accepting responsibility for what happens. That mindset begins here.\nThis chapter sets the foundation for the tools that follow. Once you see your computer as a programmable partner rather than a collection of apps, learning the command line, version control, and reproducible documents becomes natural.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your Computer as a Tool for Discovery</span>"
    ]
  },
  {
    "objectID": "02-computer.html#whats-inside-your-computer",
    "href": "02-computer.html#whats-inside-your-computer",
    "title": "2  Your Computer as a Tool for Discovery",
    "section": "2.2 What’s Inside Your Computer",
    "text": "2.2 What’s Inside Your Computer\nBefore we talk to the computer through the command line, it helps to know what is inside the box. You do not need to become a hardware expert, but a few ideas will make later chapters much easier to understand.\nAt a high level, every computer used for data science has four key parts:\n\nCPU (Central Processing Unit) The CPU is the “brain” of the computer. It follows instructions one step at a time and does the general-purpose work in your programs.\nRAM (Random-Access Memory) RAM is the computer’s short-term memory. When you open a dataset in Python or R, it is copied into RAM so the CPU can work with it quickly. If you do not have enough RAM, large projects slow down or crash.\nStorage (SSD or hard drive) Storage is the long-term memory. Your files, photos, code, and datasets live here even after you shut down the computer. Solid-state drives (SSD) are faster and more reliable than older spinning hard drives. An SSD has no moving parts, which makes it faster and less fragile than older drives.\nGPU (Graphics Processing Unit) The GPU started as a special chip for drawing graphics and games. Modern data science uses GPUs for huge math problems, such as training deep learning models, because they can do many simple calculations in parallel.\n\nYou can think of the CPU as a student solving math problems, RAM as the open notebook on the desk, storage as the backpack and bookshelf, and the GPU as a team of helpers who can all work on similar problems at the same time.\n\n\n\n\n\n\nNoteAdvanced: How Much Is “Enough”?\n\n\n\n\n\nFor small school projects, almost any modern laptop will work. As you move toward bigger datasets and models, more RAM usually helps more than more CPU cores. If you often run out of memory when loading data, adding RAM or moving to a machine with more RAM makes a big difference. GPUs become important mainly for large deep learning or image-heavy projects.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your Computer as a Tool for Discovery</span>"
    ]
  },
  {
    "objectID": "02-computer.html#operating-systems",
    "href": "02-computer.html#operating-systems",
    "title": "2  Your Computer as a Tool for Discovery",
    "section": "2.3 Operating Systems",
    "text": "2.3 Operating Systems\nAn operating system (OS) is the layer of software that connects your programs to the hardware. It controls files, devices, memory, and how programs run. The three main families you will see in data science are Windows, macOS, and Linux.\n\nLinux Linux is the standard in professional data science and on almost all servers and cloud machines. It is predictable: nothing major happens behind your back. When you install software or change a setting, it stays that way until you change it again. The command line tools you learn in this book behave the same way on almost every Linux system.\nmacOS macOS is based on Unix, like Linux, and includes a built-in terminal that understands almost all the same commands. Many data scientists who use laptops choose macOS because it balances a friendly interface with powerful command line tools.\nWindows Windows makes it easy to start, but it also tries to “help” by hiding file extensions, changing paths, and running background tools that can confuse reproducible work. Different parts of Windows sometimes disagree about how to name files or run commands. These shortcuts can be convenient for everyday use but can teach habits that do not transfer well to Linux or the cloud.\n\nIn this book, we will prefer a Unix-style command line everywhere. On macOS and Linux, that means using the built-in Terminal app. On Windows, that means installing and using Git Bash, which brings a Unix-like terminal to Windows so that the same commands work on all three platforms.\nA predictable system is easier to master. A system that behaves exactly as instructed is easier to debug. In data science, predictability matters more than convenience.\n\n2.3.1 Installing Software from the Command Line\nData scientists often install tools using package managers, which are programs that download, install, and update software for you from the command line. This is faster, more repeatable, and easier to document than clicking through many installer windows.\nHere are the most common options:\n\nLinux Each Linux distribution comes with its own package manager. For example: apt on Ubuntu, dnf on Fedora, and pacman on Arch. These tools let you install almost everything you need with a single command.\nmacOS On macOS, the most popular package manager is Homebrew. After installing Homebrew once, you can run commands like brew install git or brew install python to get new tools.\nWindows On Windows 10 and 11, you can use winget from the command line to install software in Windows Shells (PowerShell or CMD). For example, yo can install Git Bash with winget install Git.Git. Git Bash is a Windows terminal that lets you use the same command-line tools and Git commands commonly used on Linux and macOS. However, winget is a Windows-only tool and should be run in PowerShell or Command Prompt, not in Git Bash.\n\n\n\n\n\n\n\nNoteAdvanced: Other Options on Windows\n\n\n\n\n\nSome developers use full Linux environments on Windows through the Windows Subsystem for Linux (WSL) or alternative package managers such as Chocolatey. These are powerful options when you do a lot of development on Windows, but you do not need them for this book.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your Computer as a Tool for Discovery</span>"
    ]
  },
  {
    "objectID": "02-computer.html#how-computers-represent-numbers",
    "href": "02-computer.html#how-computers-represent-numbers",
    "title": "2  Your Computer as a Tool for Discovery",
    "section": "2.4 How Computers Represent Numbers",
    "text": "2.4 How Computers Represent Numbers\nComputers are built on binary, a number system that uses only zeros and ones. That design works well for storing whole numbers, but it creates some surprises when we work with decimals.\nThere are two basic kinds of numbers you will see in data science:\n\nIntegers (…, -2, -1, 0, 1, 2, …) These are whole numbers with no decimal part. Computers can store many integers exactly.\nFloating-point numbers (like 0.1, 2.75, or -3.14) These are used for decimals and measurements. Most real-valued data in science and statistics are stored as floating-point numbers.\n\nBecause computers use binary, many simple-looking decimals cannot be stored exactly. For example, the decimal number 0.1 turns into a long repeating pattern in binary. The computer stores a very close approximation instead of the exact value. When you combine many such numbers, the tiny differences can show up as small rounding errors.\nYou may have seen examples where a language reports that 0.1 + 0.2 is 0.30000000000000004 instead of exactly 0.3. This is not a bug in Python or R. It is a consequence of how floating-point numbers are stored in hardware. Data scientists work with this by rounding results for display and by avoiding direct equality checks with decimals.\nBelow is a simple demonstration in Python.\n\n0.1 + 0.2 == 0.3\n\nFalse\n\n\n\n0.1 + 0.2\n\n0.30000000000000004\n\n\n\nround(0.1 + 0.2, 10) == 0.3\n\nTrue\n\n\nThe key ideas to remember are:\n\nSome decimals cannot be represented exactly on a computer.\nSmall rounding differences are normal in real-number calculations.\nWe usually care about being “close enough” rather than perfectly exact.\n\n\n2.4.1 Integer overflow\nFloating-point numbers are not the only source of surprises. Integers can also behave in unexpected ways when they are stored using a fixed number of bits.\nIn many systems, integers are stored with a fixed width, such as 8 bits, 16 bits, 32 bits, or 64 bits. When a number becomes too large to fit in that fixed space, it wraps around. This is called integer overflow.\nPython’s built-in int type can grow automatically. However, many data science libraries use fixed-size integers for efficiency.\n\nimport numpy as np\n\nx = np.int8(127)\nx, x + 1\n\n/var/folders/cq/5ysgnwfn7c3g0h46xyzvpj800000gn/T/ipykernel_13990/3977345532.py:4: RuntimeWarning: overflow encountered in scalar add\n  x, x + 1\n\n\n(np.int8(127), np.int8(-128))\n\n\nAn 8-bit signed integer can store values from -128 to 127. When you add 1 to 127, the value wraps around. The program does not crash. It continues with a new value.\nMastery means understanding not only commands, but limits.\n\n\n\n\n\n\nNoteAdvanced: Why This Matters in Data Science\n\n\n\n\n\nThe standard format for floating-point numbers in most languages is called IEEE 754. It trades exactness for speed and a wide range of values. When you compare floating-point results across languages or machines, tiny differences are expected. When results must be exactly reproducible bit-for-bit, experts sometimes use special libraries, exact arithmetic, or careful control of the hardware and compiler settings.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your Computer as a Tool for Discovery</span>"
    ]
  },
  {
    "objectID": "02-computer.html#the-command-line-speaking-your-computers-language",
    "href": "02-computer.html#the-command-line-speaking-your-computers-language",
    "title": "2  Your Computer as a Tool for Discovery",
    "section": "2.5 The Command Line: Speaking Your Computer’s Language",
    "text": "2.5 The Command Line: Speaking Your Computer’s Language\nMost people use computers through windows, buttons, and icons. The command line offers a different approach: you type short instructions that the computer understands directly. This way of working gives you transparency, repeatability, and control that clicking cannot provide. Data science relies on these qualities because your work must be clear, sharable, and reproducible. On Windows, you will use Git Bash; on macOS or Linux, you will use the Terminal app.\nThe command line does nothing on its own. It waits for you. That can feel uncomfortable at first. Some students are hesitant the first time they open a terminal window because there are no visible buttons to guide them. That reaction is normal. With practice, the clarity becomes empowering.\n\n2.5.1 Instructing Your Computer through Command Line\nThe command line is an interface where you communicate with the computer by typing commands. Each command performs one well-defined action. Because every action appears plainly on the screen, the command line makes your steps visible and traceable. This transparency helps you understand what you are doing, and it allows others to follow your work. In contrast, pointing and clicking through mouses leaves no reliable record. The command line also supports automation: a command that you type once can be saved in a script and reused whenever you need it. This repeatability is a cornerstone of reproducible data science.\nWhen you write commands in a script and save them in a file, the computer can repeat those steps exactly. If you make a mistake, you can correct the script and run it again. If a friend wants to understand your analysis, you can send them the script instead of trying to describe what you clicked. This practice of scripting is at the heart of reproducible data science.\nThe command line also gives you access to tools that do not have a graphical interface at all. Many powerful utilities, including Git for version control and Quarto for reproducible documents, are designed to be run from the terminal. Learning the command line opens the door to these tools and lets you combine them in flexible ways.\nAt first, the command line may feel slower than clicking. That feeling fades as you learn the basic commands. Eventually, you will be able to move through folders, manage files, and run complex workflows with just a few keystrokes. Small scripts you write today can become the building blocks for larger projects later.\nBecoming comfortable here is often a turning point. When you realize that you can direct the system precisely, you begin to see your computer not as a mystery, but as a tool you control.\n\n\n2.5.2 Navigating Your System\nWhen you open Git Bash or Terminal, you start in a current working directory — the folder where the computer assumes you want to work. The commands you type will use this folder unless you tell them otherwise. To use the command line effectively, you need to know where you are and how to move around.\nThe key commands for navigation are:\n\npwd — print working directory (shows your current folder)\nls — list files and folders in the current directory\ncd &lt;path&gt; — change directory to the folder given by &lt;path&gt;\n\nPaths come in two flavors:\n\nAbsolute paths start from the root of the file system and show the full route to a folder.\nRelative paths start from your current location and describe how to get somewhere from there.\n\nFor example, on a typical system your home directory might be something like /Users/alex on macOS or Linux, or C:\\Users\\alex on Windows. In Git Bash, the Windows path will appear in a Unix-style form such as /c/Users/alex. If you are in your home directory and you want to move to a subfolder called projects, you can run cd projects. If you are somewhere else and want to jump straight to your home directory, you can use cd ~.\nThe .. symbol means “the parent directory” — the folder that contains the one you are in:\n\ncd .. moves you up one level.\ncd ../.. moves you up two levels.\n\nThe . symbol means “the current directory”. You will often see it when running programs that should use the current folder as their starting point.\nAs you practice, pay attention to the prompt in your terminal. It often shows your current directory or at least the last part of the path. This small detail helps you keep track of where you are working.\nWhen you always know where you are in the file system, you reduce mistakes. Precision begins with location.\n\n\n2.5.3 Managing Files and Folders\nThe command line also lets you create, move, and delete files and folders. At first, these actions may feel risky, but they quickly become a precise way to organize your projects.\nCommon commands include:\n\nmkdir &lt;name&gt; — create a new folder\ntouch &lt;filename&gt; — create an empty file\nrm &lt;filename&gt; — remove a file\nrm -r &lt;folder&gt; — remove a folder and everything inside\ncp &lt;source&gt; &lt;destination&gt; — copy a file\nmv &lt;old&gt; &lt;new&gt; — rename or move a file\n\nBefore removing anything, confirm your location with pwd and ls. The command line will execute exactly what you type.\nClear and consistent naming makes your work easier to understand and avoids errors later, especially when your projects grow. Good naming practices for data science include:\n\nUse lower case whenever possible (data/, not Data/).\nAvoid spaces, which cause trouble in the terminal (raw_data, not raw data).\nUse dashes or underscores, but pick one and stay consistent (weather-data or weather_data).\nAvoid special characters such as !, ?, *, #, or &.\nPrefer short, descriptive names (scripts/, figures/, clean.py).\nOrganize by purpose, not by date alone. Use folders like data/, scripts/, projects/, and output/.\nKeep related files together, and avoid scattering pieces of the same project across unrelated locations.\n\nGood naming makes your work predictable—for you, your future self, and anyone you collaborate with. It also reduces mistakes when writing paths or running scripts from the terminal.\nNaming is part of mastery. When your file structure is clear, your thinking is clearer too.\n\n\n2.5.4 Running Programs from the Terminal\nThe command line can also start programs and check whether your tools are installed correctly. Examples include:\n\ncodium . — open the current folder in VSCodium\ngit --version — check that Git is installed\npython --version — check your Python installation\nR --version — confirm that R is available\nquarto check — verify that Quarto is installed correctly\n\nLaunching programs from the terminal reinforces the idea that your computer is programmable. It also prepares you for workflows where scripts and tools need to run together smoothly.\n\n\n2.5.5 Mini-Project: Creating Your First Data Science Workspace\nTo make these ideas concrete, you will now create a simple workspace for your future projects.\n\nOpen the Terminal On Windows, start Git Bash. On macOS or Linux, open the Terminal app.\nFind Your Home Directory Run pwd to display your current working directory. If you are not already in your home directory, move there using cd ~.\nMake a my_project Folder Create a directory named my_project with mkdir my_project. Move into it using cd my_project. Confirm that you are in the right place by running pwd.\nAdd a Few Subfolders Inside my_project, create directories named data, analysis, and figures. Check your work with ls to ensure they appear.\nPractice Moving Around Change into the data directory with cd data. Move back to my_project with cd ... Try moving directly to figures using cd figures. Move up one level with cd ... Return to the previous directory using cd -.\nOpen a Project Folder in VSCodium From inside the my_project directory, open the project in VSCodium by running code . (If necessary, enable the “code” command in VSCodium.)\nCreate a Small Project Template Inside my_project, create a folder named mini_project. Within it, create subfolders data, analysis, and figures. Add an empty Quarto file using touch analysis/analysis.qmd and an empty CSV file using touch data/example.csv. Use ls -R to display the full directory structure.\n\nMastery grows from repeated small, precise actions.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your Computer as a Tool for Discovery</span>"
    ]
  },
  {
    "objectID": "02-computer.html#exercises",
    "href": "02-computer.html#exercises",
    "title": "2  Your Computer as a Tool for Discovery",
    "section": "2.6 Exercises",
    "text": "2.6 Exercises\n\nOpen Git Bash or Terminal and run pwd. Write down the full path and circle the folder you are currently in.\nUse ls to display the files and folders in your working directory. Then run ls -l and note at least two new pieces of information you see compared with ls.\nNavigate to your home directory using cd ~. Verify your location using pwd.\nNavigate to your Downloads folder and verify your location using pwd.\nOn macOS or Linux:\ncd ~/Downloads\npwd\nOn Windows (PowerShell):\ncd $HOME/Downloads\npwd\nNavigate to your home directory using cd ~. Create a folder named my_project with mkdir my_project. Move into it with cd my_project and verify your location using pwd.\nInside my_project, create subfolders for data, analysis, and figures. Use ls to confirm that the folders were created. Practice moving between them with cd, cd .., and cd -.\nFrom inside the my_project directory, open the project in VSCodium by running code .. In the VSCodium terminal, run ls -R to display the full directory structure.\nInside my_project, create a folder named mini_project with subfolders data, analysis, and figures. Add an empty Quarto file using touch analysis/analysis.qmd and an empty CSV file using touch data/example.csv. Use ls -R mini_project to check that everything is in the right place.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Your Computer as a Tool for Discovery</span>"
    ]
  },
  {
    "objectID": "03-tools.html",
    "href": "03-tools.html",
    "title": "3  Right Tools for Data Science Projects",
    "section": "",
    "text": "3.1 Introduction\nThis chapter introduces the tools required to carry out reproducible data science projects in this course. A reproducible project is one in which the full analysis — data inputs, code, narrative, and results — can be rerun by someone else, or by the same analyst at a later time, with the same outcomes. Reproducibility is therefore a property of an entire project rather than of any single tool. The goal of this chapter is not to teach tools in isolation, but to explain how a small set of tools work together to support this project-level reproducibility.\nWithout a structured workflow, data science work tends to become fragile. Files are renamed or overwritten, intermediate results are lost, and analysis steps are no longer clear even to the original author. These problems become more severe as projects grow in size or involve collaboration. In an academic setting, they also make it difficult to review, grade, or extend work. The tools introduced in this chapter are motivated by these common failure modes and are chosen to prevent them in a systematic way.\nThe tools in this chapter form a coherent workflow rather than a loose collection of software. Git records the history of a project and makes changes explicit. GitHub provides a shared location for storing, submitting, and reviewing projects. Quarto allows code, text, and results to live in a single executable document. Python, R, or Julia provides the computational engine for data analysis. VS Codium serves as a coordinating environment that brings these tools together in daily work. Each tool plays a distinct role, and reproducibility emerges from their interaction.\nThis chapter includes parallel setup sections for Python, R, and Julia. You only need to choose one language and follow that section. If you already know one of these languages, start there. Otherwise, Python, R, and Julia are all reasonable choices. Quarto can technically execute code cells written in different languages within the same document, but for clarity and consistency you are expected to work primarily in a single language. Once your chosen language is set up, you can skip the other language sections; the remainder of the book assumes a single-language workflow rather than switching between languages.\nAll work produced using this setup is treated as a reproducible project. Files are organized in a clear directory structure, tracked with version control, and rendered into executable documents that combine narrative, code, and results. Screenshots, manually edited outputs, and undocumented analysis steps are not substitutes for this workflow. Once learned, this way of working is not limited to data science courses: the same tools can be used to manage assignments in other classes, maintain a research diary, write a blog, draft a novel, or even organize personal notes and creative writing. By the end of this chapter, you should be able to set up a complete project, make changes in a controlled way, and produce work that can be reliably revisited, reused, and shared.\nIn the rest of this chapter, tools are introduced in the order you need them in a real workflow: command line first, then a language, then Quarto, then Git, then an editor, and finally GitHub.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Right Tools for Data Science Projects</span>"
    ]
  },
  {
    "objectID": "03-tools.html#the-command-line-is-the-foundation",
    "href": "03-tools.html#the-command-line-is-the-foundation",
    "title": "3  Right Tools for Data Science Projects",
    "section": "3.2 The Command Line Is the Foundation",
    "text": "3.2 The Command Line Is the Foundation\nThis chapter assumes you have already learned the basics of the command line (also called the shell) earlier in the book. The command line is the foundation for the entire tooling workflow, because it is the same on any computer and it works with any editor.\n\n3.2.1 Why the command line still matters\nFor many beginners, the command line feels old-fashioned compared to graphical menus. In practice, it remains central to technical work for several reasons.\n\nSpeed. Typing commands is often faster than navigating menus. Autocompletion means you rarely type full filenames or commands.\nPrecision. A command specifies exactly which program and which files are used. There is no ambiguity about what happened.\nReproducibility. Commands can be written down, copied, and rerun. Mouse clicks cannot be reliably reproduced.\nUniversality. The same commands work in a regular terminal, on a remote server, or inside an editor such as VSCodium.\nProfessional practice. In movies and television, technical experts are almost always shown typing commands rather than clicking icons. This reflects reality: serious technical work rewards tools that favor speed, precision, and repeatability.\n\nThroughout this chapter, you will repeatedly do three things:\n\nNavigate into a project folder\nRun a program (Git, Quarto, Python/R/Julia)\nCheck what happened (files created, versions, error messages)\n\nWhen you install a tool, you should also verify it from the command line. A typical verification pattern is:\ntoolname --version\nFor example:\ngit --version\nquarto --version\npython --version\nR --version\njulia --version\nYou do not need to memorize commands. The goal is to build the habit of checking what is installed and what version you are using.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Right Tools for Data Science Projects</span>"
    ]
  },
  {
    "objectID": "03-tools.html#the-workflow-before-the-tools-a-conceptual-preview",
    "href": "03-tools.html#the-workflow-before-the-tools-a-conceptual-preview",
    "title": "3  Right Tools for Data Science Projects",
    "section": "3.3 The Workflow Before the Tools (A Conceptual Preview)",
    "text": "3.3 The Workflow Before the Tools (A Conceptual Preview)\nBefore installing anything, it helps to see the full workflow at a high level. Reproducible data science is not one tool. It is a small system of tools that work together.\nA typical workflow looks like this:\n\nWrite a Quarto file (.qmd) that contains text and code\nRun the code using your chosen language (Python, R, or Julia)\nRender the Quarto file into an output document (usually HTML)\nUse Git to record snapshots of your work (commits)\nUse GitHub to share the work and collaborate\n\nThe transcript below is a preview. Do not try to run it yet. You will install the tools and run these commands later, step by step.\n# create a project folder\nmkdir my-project\ncd my-project\n\n# write a Quarto file (in an editor)\n# then render it\nquarto render analysis/report.qmd\n\n# record your work with Git\ngit init\nmkdir analysis\nnotepad analysis/report.qmd\n\ngit add analysis/report.qmd\ngit commit -m \"First report\"\n\n# share your work on GitHub (after setup)\ngit remote add origin &lt;url&gt;\ngit push -u origin main\nIf you understand the purpose of each line in this transcript by the end of the chapter, you have learned the core tooling workflow.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Right Tools for Data Science Projects</span>"
    ]
  },
  {
    "objectID": "03-tools.html#gate-1-choose-a-programming-language-and-set-up-its-environment",
    "href": "03-tools.html#gate-1-choose-a-programming-language-and-set-up-its-environment",
    "title": "3  Right Tools for Data Science Projects",
    "section": "3.4 Gate 1: Choose a Programming Language and Set Up Its Environment",
    "text": "3.4 Gate 1: Choose a Programming Language and Set Up Its Environment\nIn this book, you will write code in one programming language. Choose exactly one language to start (Python, R, or Julia), and follow the installation instructions for that language only. You can always learn a second language later.\nThis gate has one goal: you should be able to run a short piece of code from the command line.\n\n3.4.1 Python\nPython is the most widely used programming language in modern data science and an excellent first language for beginners. Its clean, readable syntax allows students to focus on ideas rather than punctuation, while its large ecosystem means that most common tasks already have well-tested libraries. In this course, Python will be used to write code, analyze data, and produce graphics. These skills form the foundation for reproducible data science workflows introduced later in the book.\nPython has earned its central role because it balances power and accessibility. The base language is expressive and easy to read, and many core tools are included by default. Beyond that, widely used libraries such as pandas for data analysis, matplotlib for visualization, and scikit-learn for machine learning allow students to move quickly from simple examples to realistic projects. While R is also an excellent language, especially for statistics, Python will be the default language in this book, with R treated as an optional companion.\n\n3.4.1.1 Installing Python (Python 3.12)\nWe will use Python 3.12. At the time of writing, this version is supported by major scientific and machine-learning libraries, including TensorFlow and PyTorch. From the beginning, we use the command line to install and manage software so that the workflow is transparent and reproducible.\nChoose the instructions for your operating system.\nWindows (using winget)\nOpen PowerShell (not Git Bash) and run:\nwinget install -e --id Python.Python.3.12\nAfter installation, close and reopen your terminal so that python is available on the PATH.\nmacOS (using Homebrew)\nIf Homebrew is not installed, follow the instructions at https://brew.sh.\nThen install Python 3.12 by running:\nbrew install python@3.12\nHomebrew installs Python in a standard location and makes it available from the terminal.\nLinux (Debian / Ubuntu using apt)\nThese instructions assume a Debian- or Ubuntu-based system.\nFirst update package information:\nsudo apt update\nInstall Python 3.12 and the virtual environment module:\nsudo apt install python3.12 python3.12-venv\nVerify the installation:\npython3.12 --version\nYou should see output similar to:\nPython 3.12.x\nIf this command fails, resolve the installation before continuing.\n\n\n3.4.1.2 Creating a virtual environment\nVirtual environments should be created in a deliberate and consistent location. We recommend creating a dedicated directory in your home folder to store environments for this course.\nFrom a terminal, create a directory named envs in your home directory:\nmkdir -p ~/envs\nCreate a virtual environment named ds-env inside this directory:\npython -m venv ~/envs/ds-env\nActivate the environment:\n\nWindows (Git Bash):\nsource ~/envs/ds-env/Scripts/activate\nmacOS / Linux:\nsource ~/envs/ds-env/bin/activate\n\nWhen the environment is active, your prompt will usually change to show (ds-env).\nUpgrade the package manager and install a small set of core libraries:\npython -m pip install --upgrade pip\npip install numpy pandas matplotlib\nAll Python packages for this course should be installed inside this environment.\nTo leave the environment, run:\ndeactivate\n\n\n3.4.1.3 A short Python warm-up\nBefore moving on, you should be comfortable with:\n\nrunning Python from the command line,\nwriting and executing simple scripts,\nusing variables, lists, and loops,\nimporting and using packages.\n\nFor a concise, official warm-up that can be completed in 2–3 hours, use the following resources from the Python Documentation:\n\nAn Informal Introduction to Python: https://docs.python.org/3/tutorial/introduction.html\nMore Control Flow Tools: https://docs.python.org/3/tutorial/controlflow.html\nModules: https://docs.python.org/3/tutorial/modules.html\n\nRead these sections in order. They provide just enough structure to begin experimenting with Python and to learn additional features on demand.\nAfter completing this warm-up, you should be ready to work interactively and write small scripts. The next section builds on this foundation to introduce a reproducible data science workflow.\n\n\n\n\n\n\n\n\n\n\n\n\n3.4.2 R\nR is a programming language designed for statistical computing and graphics. It is widely used in statistics, biostatistics, and the parts of data science that emphasize modeling, inference, and visualization. R also has an unusually strong culture of packaging and documentation, which makes it well suited for project-based work where others need to rerun and review an analysis.\nR is often preferred when the workflow depends on statistical modeling, publication-quality graphics, or domain-specific methods that are most mature in the R ecosystem. In practice, many projects use both Python and R, selecting the language based on the task rather than loyalty to a tool. This book uses Python as the default engine, but the workflow in this chapter applies equally well to R.\nTo keep R-based projects reproducible, you must manage two things carefully: the R version and the set of R packages used by the project. The installation steps below install R, and the environment steps show how to record and restore project-specific package versions.\n\n3.4.2.1 Installing R (R 4.5)\nChoose the instructions for your operating system.\nWindows (using winget)\nOpen PowerShell (not Git Bash) and run:\nwinget install -e --id RProject.R\nAfter installation, close and reopen your terminal.\nVerify that R is available:\nR --version\nRscript --version\nIf R is not found, it is usually because the installer did not add R to your PATH. In that case, locate your R installation (often under C:\\\\Program Files\\\\R\\\\) and either run R.exe / Rscript.exe from that folder or add the appropriate bin directory to your PATH.\nmacOS (using Homebrew)\nInstall R:\nbrew install r\nVerify:\nR --version\nRscript --version\nLinux (Debian / Ubuntu using apt)\nFor a straightforward installation, use your system package manager:\nsudo apt update\nsudo apt install r-base r-base-dev\nVerify:\nR --version\nRscript --version\nIf your distribution provides an older R than you need, install from the CRAN-maintained Ubuntu repository and follow the current instructions in the CRAN “Ubuntu Packages For R” guide.\n\n\n3.4.2.2 Creating a project library with renv\nR packages are installed into libraries, which are directories on disk. If you install packages globally, different projects can silently share (and overwrite) the same dependencies. This is convenient at first, but it eventually breaks reproducibility when package versions change.\nFor R projects in this book, use renv to manage a per-project package library. renv creates a project-local library and records dependency versions in a lockfile (renv.lock). Anyone with the same R version can restore the project library from that lockfile.\nFirst, install renv (once per machine).\nR -q -e \"install.packages('renv')\"\nThen, from the root directory of an R project, initialize renv:\nR -q -e \"renv::init()\"\nInstall core packages used in many projects:\nR -q -e \"install.packages(c('ggplot2', 'dplyr', 'readr'))\"\nSnapshot the environment to update renv.lock:\nR -q -e \"renv::snapshot()\"\nOn another machine (or after deleting the project library), restore the environment from the lockfile:\nR -q -e \"renv::restore()\"\nIn a Git repository, commit renv.lock and the renv/ infrastructure files created by renv. Do not commit the installed package binaries in renv/library/ unless you have a specific reason to do so.\n\n\n3.4.2.3 A short R warm-up\nBefore moving on, you should be comfortable with:\n\nstarting R from the command line,\nrunning one-line commands with R -e,\nwriting and running scripts with Rscript,\nusing vectors, lists, and data frames,\nloading packages and reading help pages.\n\nFor an official warm-up that can be completed in a few hours, use the R Core Team manual “An Introduction to R” and work through the sections on objects, data structures, and graphics. As you work, practice using help() and help.search() to find documentation from within R.\nAfter completing this warm-up, you should be ready to write small R scripts and to render Quarto documents using R as the execution engine.\n\n\n\n3.4.3 Julia\nJulia is a programming language designed for numerical and scientific computing. It combines a high-level, expressive syntax with performance that is often comparable to low-level languages such as C or Fortran. This design makes Julia attractive for simulation-heavy workloads, optimization, and research code where clarity and speed are both important.\nJulia is increasingly used in data science and applied statistics when projects involve custom algorithms, large simulations, or performance- critical components that would be cumbersome to write in other high-level languages. It is less ubiquitous than Python or R, but its package ecosystem is mature enough for many modeling, visualization, and data manipulation tasks.\nAs with Python and R, reproducibility in Julia depends on controlling the language version and the exact versions of packages used. Julia was designed with this in mind: its built-in package manager records full dependency graphs for each project, making environment management a first-class feature rather than an add-on.\n\n3.4.3.1 Installing Julia (1.x series)\nChoose the instructions for your operating system.\nWindows (using winget)\nOpen PowerShell (not Git Bash) and run:\nwinget install -e --id JuliaLang.Julia\nAfter installation, close and reopen your terminal.\nVerify:\njulia --version\nmacOS (using Homebrew)\nInstall Julia:\nbrew install julia\nVerify:\njulia --version\nLinux\nDownload the official Linux binary from the Julia website and extract it to a convenient location, such as /opt/julia or your home directory. Then add the bin directory to your PATH.\nVerify:\njulia --version\nIf julia is not found, check that the directory containing the Julia binary is on your PATH.\n\n\n3.4.3.2 Project environments with Julia’s package manager\nJulia uses project environments to isolate dependencies. Each project is associated with a Project.toml file that lists direct dependencies and a Manifest.toml file that records the exact versions of all packages, including transitive dependencies.\nTo create or activate a project environment, navigate to the project root directory and start Julia:\njulia\nAt the Julia prompt, activate a local environment:\n]\nactivate .\nThe closing bracket ] switches Julia into package manager mode. The command activate . tells Julia to use a project environment stored in the current directory.\nAdd commonly used packages:\n]\nadd DataFrames CSV Plots\nThis creates (or updates) Project.toml and Manifest.toml in the project directory. These files fully specify the environment.\nOn another machine, or after cloning the repository, activate the project and instantiate the environment:\n]\nactivate .\ninstantiate\nIn a Git repository, commit both Project.toml and Manifest.toml. These files play the same role as requirements.txt or renv.lock in other languages, but with stricter guarantees about reproducibility.\n\n\n3.4.3.3 A short Julia warm-up\nBefore moving on, you should be comfortable with:\n\nstarting Julia from the command line,\nusing the Julia REPL and its help system,\nactivating and instantiating project environments,\nworking with arrays, dictionaries, and tables,\nloading packages with using and import.\n\nA concise and authoritative starting point is the official Julia manual “Getting Started” and the sections on the REPL, packages, and performance tips. After this warm-up, you should be able to write small Julia scripts and use Julia as an execution engine in Quarto documents when a project benefits from Julia’s performance and numerical strengths.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Right Tools for Data Science Projects</span>"
    ]
  },
  {
    "objectID": "03-tools.html#gate-2-quarto-reproducible-documents-for-real-data-science",
    "href": "03-tools.html#gate-2-quarto-reproducible-documents-for-real-data-science",
    "title": "3  Right Tools for Data Science Projects",
    "section": "3.5 Gate 2: Quarto (Reproducible Documents for Real Data Science)",
    "text": "3.5 Gate 2: Quarto (Reproducible Documents for Real Data Science)\nNow that you can run code in your chosen language, you can use Quarto to combine code, text, and results in one reproducible document.\n\n3.5.1 Why Quarto?\nQuarto is a tool for writing documents that combine text, code, figures, and results in a single, executable source file. Instead of keeping separate word-processor files, exported plots, screenshots, and loose scripts, Quarto keeps analysis and narrative together and regenerates results automatically whenever code or data change. This makes work transparent, reproducible, and easier to review.\nQuarto is widely used in data science for technical reports, notebooks, and presentations where correctness and traceability matter. Compared to traditional notebooks, Quarto emphasizes documents first and interactivity second. Compared to Word or PowerPoint, it prioritizes reproducible computation over manual formatting. These properties make Quarto well suited for teaching and for real data science projects, where reasoning and evidence are more important than appearance.\n\n\n3.5.2 Installation and setup (CLI-first)\nQuarto is a command-line tool. We install and use it from the terminal, independent of any editor.\nWindows (using winget)\nOpen PowerShell and run:\nwinget install --id Posit.Quarto -e\nClose and reopen the terminal after installation.\nmacOS (using Homebrew)\nbrew install quarto\nLinux (Debian / Ubuntu)\nsudo apt update\nsudo apt install quarto\nAfter installation, verify that Quarto is available:\nquarto check\nThis command reports available engines (such as Python) and confirms that Quarto is correctly installed.\nEditor support (optional)\nQuarto works entirely from the command line. If you use VSCodium, you may optionally install the Quarto extension for syntax highlighting and convenience features. This is not required for rendering documents.\n\n\n3.5.3 Quarto and Python environments\nQuarto does not manage Python installations or virtual environments. When rendering a document, it uses the Python interpreter available on your PATH.\nFor this course, you must activate your course environment before running Quarto commands.\nsource ~/envs/ds-env/bin/activate\nOn Windows (Git Bash):\nsource ~/envs/ds-env/Scripts/activate\nAfter activation, verify:\npython --version\nQuarto will now execute all Python code chunks using this environment.\n\n\n3.5.4 Anatomy of a Quarto file\nA Quarto document (.qmd) has three main components, executed from top to bottom during rendering.\nYAML header\nThe YAML header appears at the top of the file between --- lines and controls document-level settings such as title and output format.\n---\ntitle: \"My Document\"\nformat: html\n---\nMarkdown body\nThe body contains text written in Markdown: headings, paragraphs, lists, links, and mathematical notation. This is where you explain your analysis and interpret results.\nCode chunks\nCode chunks contain executable code. During rendering, Quarto runs the code and inserts the output directly into the document. In this course, we use Python by default.\nprint(“Hello, Quarto”)\n\n\n3.5.5 First reproducible notebook\nA minimal workflow for your first Quarto document is:\n\ncreate a source file,\nwrite text and code together,\nrender the document from the command line.\n\nCreate a new file named my_first.qmd with the following header:\n---\ntitle: \"my-first-quarto-notebook\"\nformat: html\n---\nAdd a short paragraph:\nThis document demonstrates a simple, reproducible analysis written in\nPython using Quarto.\nBefore running this example, make sure the required plotting library is installed in your active environment:\npip install matplotlib\nInsert a Python code chunk that produces a figure:\nimport matplotlib.pyplot as plt\nplt.plot([1, 2, 3], [1, 4, 9]) plt.title(“A Simple Plot”) plt.show()\nActivate your environment and render:\nsource ~/envs/ds-env/bin/activate\nquarto render my_first.qmd\nThis produces an HTML file that contains the text, code, and generated output.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Right Tools for Data Science Projects</span>"
    ]
  },
  {
    "objectID": "03-tools.html#gate-3-git-local-version-control-for-reproducible-projects",
    "href": "03-tools.html#gate-3-git-local-version-control-for-reproducible-projects",
    "title": "3  Right Tools for Data Science Projects",
    "section": "3.6 Gate 3: Git (Local Version Control for Reproducible Projects)",
    "text": "3.6 Gate 3: Git (Local Version Control for Reproducible Projects)\nGit comes after you can run code and render a document, because Git is most useful when it records real work: your source files and the outputs you choose to keep.\n\n3.6.1 Why Git?\nVersion control is a foundational skill for data science because it treats your work as a living project with a complete history. Git lets you track every change you make, recover from mistakes, and collaborate without overwriting anyone’s work. Unlike saving multiple file versions by hand (e.g., project_final_v12_REAL), Git provides a precise, automatic timeline of your edits. This makes your work reproducible, auditable, and shareable, which are essential habits for scientific computing and data science projects.\nGit is used to manage data science projects as evolving artifacts rather than one-time snapshots. Analyses typically change as data are cleaned, models are revised, and interpretations improve. A Git repository should contain only the files needed to understand, reproduce, and extend a project—nothing more.\n\n\n3.6.2 Installing Git\nGit must be installed before it can be used. Use the method appropriate for your system.\n\nWindows (PowerShell):\n\nwinget install --id Git.Git -e\nThis installs Git together with Git Bash, a terminal environment used throughout this book. Do not run winget inside Git Bash.\n\nmacOS (Homebrew):\n\nbrew install git\n\nDebian / Ubuntu Linux (including WSL):\n\nsudo apt update\nsudo apt install git\nAfter installation, configure Git once so your work is properly attributed:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"your@email.com\"\nVerify that Git is installed and configured correctly:\ngit --version\ngit config --list\n\n\n3.6.3 Essential Git Commands\nGit is most effective when you stage files deliberately and keep the repository clean. The following commands form the core of everyday Git usage in data science projects.\n\ngit init initializes a repository in a project folder.\ngit status shows which files have changed.\ngit add &lt;file&gt; stages selected files to be recorded.\ngit commit -m \"message\" saves a snapshot of staged changes.\ngit diff displays differences between versions.\ngit push sends committed changes to a remote repository, such as one hosted on GitHub.\ngit pull retrieves updates from a remote repository, such as one hosted on GitHub.\n\nThese two commands are where Git connects to an online platform. A common example of such a platform is GitHub, which serves as a shared location for backing up work and collaborating on projects. The next section focuses on GitHub itself and how it is used in our workflow.\nAvoid using git add . as a default habit. It often stages generated files, temporary outputs, or other unintended content. Staging files explicitly helps keep repositories small, readable, and reproducible.\n\n\n3.6.4 Basic Workflow\nGit works best when your project is organized as a single folder containing scripts, Quarto files, and documentation. A typical workflow follows a simple cycle: edit files, inspect what changed, stage selected files, and commit them with a short message explaining what was done.\ncd my-project\ngit init\ngit status\ngit add README.md\ngit add analysis.qmd\ngit commit -m \"initial project structure\"\nGit can also connect your local project to a remote repository hosted online. This idea is introduced above when discussing git push and git pull, and it is developed in detail in the next section, which focuses on GitHub as a concrete example of an online hosting platform.\n\n\n3.6.5 Good Practices for Git\nGood Git usage is less about memorizing commands and more about developing clean, repeatable habits. The following practices are especially important for reproducible data science projects:\n\nKeep the repository clean. Track only files that are needed to understand, reproduce, and extend the project. Avoid committing generated files, rendered outputs, large raw datasets, or temporary artifacts unless they are explicitly required.\nStage files deliberately. Add files explicitly rather than staging everything at once. This helps prevent accidental inclusion of unnecessary or generated content.\nCommit small, logical changes. Each commit should represent a coherent step in the project, making the history easier to read and reason about.\nWrite informative commit messages. A short message explaining what changed and why is more valuable than a vague description.\nUse .gitignore consistently. The .gitignore file is the primary mechanism for keeping a repository clean over time. It tells Git which files should never be tracked, such as temporary outputs, cache directories, and system-specific files.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Right Tools for Data Science Projects</span>"
    ]
  },
  {
    "objectID": "03-tools.html#gate-4-vscodium-an-integrated-development-environment",
    "href": "03-tools.html#gate-4-vscodium-an-integrated-development-environment",
    "title": "3  Right Tools for Data Science Projects",
    "section": "3.7 Gate 4: VSCodium: An Integrated Development Environment",
    "text": "3.7 Gate 4: VSCodium: An Integrated Development Environment\nBy this point, you can already do everything you need from the command line. An IDE does not replace the command line. It strengthens your workflow by combining editing, execution, and project organization in one environment.\n\n3.7.1 An IDE Is a Unified Workspace\nAn Integrated Development Environment (IDE) is a unified workspace for writing code, running programs, and managing projects.\nInstead of switching between a text editor, a terminal window, and a file browser, you work in one coordinated system. An IDE typically provides:\n\nSyntax highlighting (color-coding parts of code for readability)\nReal-time error detection while you type\nIntegrated terminals\nProject-wide search and navigation\nVersion control integration (such as Git)\n\nThe goal is not convenience alone. The goal is structured, reproducible work.\nCheck-in question:\nIf you can already use the command line, why might working in a unified environment reduce mistakes?\n\n\n3.7.2 Why This Book Uses VSCodium\nThis book uses VSCodium because it is fully open-source software.\nTwo closely related editors exist:\n\nVisual Studio Code (distributed by Microsoft)\nVSCodium (community-built from the same open-source code)\n\nVisual Studio Code is based on open-source code. However, the official Microsoft build includes additional proprietary components such as telemetry (automatic usage reporting) and Microsoft-specific licensing.\nVSCodium is built from the same public source code but removes proprietary components and telemetry. It is distributed under fully open-source licenses.\nThroughout this book, we choose open-source tools whenever possible. Open-source software makes its source code publicly available. Anyone can inspect it, modify it, and redistribute it under its license.\nThis matters for three reasons:\n\nTransparency — the code can be examined.\nReproducibility — the tools remain accessible.\nLongevity — projects do not depend on a single vendor.\n\nIf you already use Visual Studio Code, you may continue using it. For the workflow in this book, both behave the same.\n\n\n3.7.3 VSCodium Supports Multiple Languages and Terminals\nVSCodium is not just a text editor. It is a multi-language development environment.\nYou can:\n\nEdit Python, R, Julia, Markdown, and Quarto files\nOpen multiple terminals at the same time\nRun different shells in different terminals\nWork in several programming environments simultaneously\n\nFor example, you might:\n\nRun Python in one terminal\nRun R in another\nUse Git in a third\nEdit a Quarto document side-by-side with your analysis script\n\nAll within one window.\nThis reduces context switching and keeps your project organized.\n\n\n3.7.4 Installation\nInstall VSCodium using the method appropriate for your system.\nWindows\nInstall using winget:\nwinget install -e --id VSCodium.VSCodium\nmacOS\nInstall via Homebrew:\nbrew install --cask vscodium\nLinux\nInstall using your distribution’s package manager or the official packages provided on the VSCodium website.\nAfter installation, launch VSCodium and open the integrated terminal (View — Terminal). Confirm that it can access the tools you installed earlier:\ngit --version\nquarto --version\nAlso verify the language you chose to work with:\npython --version   # if using Python\nR --version        # if using R\njulia --version    # if using Julia\nIf these commands run successfully, VSCodium is correctly connected to your system.\n\n\n3.7.5 Extensions Connect Languages to the Editor\nExtensions allow VSCodium to understand specific languages and tools.\nInstall only what you need. A focused setup is easier to maintain.\nCore extensions (recommended for everyone):\n\nQuarto — Authoring and rendering Quarto documents\nGitLens — Enhanced Git history and comparison tools\n\nLanguage-specific extensions (choose one):\n\nPython — Editing, linting, debugging, notebook support\nR — Script editing and execution support\nJulia — Language support and code execution\n\nImportant:\nExtensions do not install Python, R, Julia, Git, or Quarto. They connect VSCodium to tools already installed on your system.\n\n\n3.7.6 The Integrated Terminal Is Part of the Workflow\nThe integrated terminal turns VSCodium into a full command-line workspace.\nOpen it with View — Terminal or Ctrl+` (Control + backtick).\nYou can open multiple terminals and select different shells for each one. This allows you to:\n\nRun Git commands\nExecute Python, R, or Julia scripts\nRender Quarto documents\nNavigate your project directory\n\nOn Windows, using Git Bash often makes commands consistent with Unix-like systems. After installing Git for Windows, open the command palette, choose Preferences: Open User Settings (JSON), and add:\n\"terminal.integrated.defaultProfile.windows\": \"Git Bash\"\nRestart the terminal after saving.\n\n\n3.7.7 Working with Projects, Not Individual Files\nIn VSCodium, you should open folders, not individual files.\nWhen you open a folder, VSCodium treats it as a project. It tracks file structure, enables Git integration, and supports project-wide search.\nUse File — Open Folder and select your project directory (for example, ds4e/).\nThis reinforces an important habit:\nData science is organized around projects, not isolated scripts.\n\n\n3.7.8 Good Practices\n\nAlways open a project folder rather than a single file\nKeep all work in plain text (scripts, notes, Quarto documents)\nUse integrated terminals for Git and language commands\nKeep all files inside your project directory\nAvoid storing active work on the desktop\n\nThese habits support clarity, organization, and reproducibility.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Right Tools for Data Science Projects</span>"
    ]
  },
  {
    "objectID": "03-tools.html#gate-5-github-hosting-and-collaboration-for-git-projects",
    "href": "03-tools.html#gate-5-github-hosting-and-collaboration-for-git-projects",
    "title": "3  Right Tools for Data Science Projects",
    "section": "3.8 Gate 5: GitHub (Hosting and Collaboration for Git Projects)",
    "text": "3.8 Gate 5: GitHub (Hosting and Collaboration for Git Projects)\nThis section assumes you already know the basics of Git on your own computer. GitHub adds sharing and collaboration on top of local Git.\n\n3.8.1 What GitHub Is and Why It Matters\nGitHub is an online platform that hosts Git repositories. Git manages project history on your computer, while GitHub stores a synchronized copy online. GitHub adds four capabilities that local Git alone does not provide.\n\nBackup: a remote copy protects against local hardware failure.\nWork anywhere: the same repository can be used on multiple machines; you can work offline and synchronize later.\nCollaboration: multiple contributors can work safely without overwriting each other.\nReview: changes can be inspected and discussed before they are merged.\n\nOnce a local repository is connected to GitHub, only two commands are needed for routine synchronization.\n\ngit push uploads committed local changes to GitHub.\ngit pull downloads changes from GitHub and integrates them locally.\n\n\n\n3.8.2 Authentication on GitHub (SSH)\nGitHub requires authentication before it accepts git push. A common method is SSH key authentication.\nSSH uses a key pair.\n\nPrivate key: stays on your computer and must never be shared.\nPublic key: added to your GitHub account to identify your computer.\n\nGenerate a key pair locally.\nssh-keygen -t ed25519 -C \"your_email@example.com\"\nThe key pair will be stored in the .ssh folder in your home directory.\nDisplay the public key.\ncat ~/.ssh/id_ed25519.pub\nCopy the public key to your clipboard.\n\nmacOS (Terminal):\n\npbcopy &lt; ~/.ssh/id_ed25519.pub\n\nWindows (Git Bash):\n\ncat ~/.ssh/id_ed25519.pub | clip\n\nWindows (PowerShell):\n\nGet-Content $env:USERPROFILE\\.ssh\\id_ed25519.pub | clip\n\nLinux / WSL:\n\nsudo apt update\nsudo apt install xclip\nxclip -selection clipboard &lt; ~/.ssh/id_ed25519.pub\nAdd the public key on GitHub.\n\nProfile menu — Settings\nSSH and GPG keys — New SSH key\nPaste the key and save\n\nTest the connection.\nssh -T git@github.com\nFor up-to-date authentication, one may ask an AI assistant for help for your operating system. For example:\n\n“How do I set up SSH keys for GitHub on Windows using Git Bash and verify that git push works?”\n\n\n\n3.8.3 Creating and Publishing a New Repository\nThis workflow applies only if you already have a local Git repository. If not, create one first with git init.\nFirst, create a new empty repository on GitHub (do not initialize it with a README). Then connect your local repository to GitHub.\ngit remote add origin git@github.com:yourname/example-repo.git\ngit branch -M main\ngit push -u origin main\nHere, yourname is your GitHub username and example-repo is the repository name you chose on GitHub.\n\n\n3.8.4 Cloning an Existing Repository\nCloning is used when the repository already exists on GitHub. Suppose that someone is the GitHub username or organization that owns a repository called project. To clone this repository to your own computer, first cd to the folder where you want put the repository and then:\ngit clone git@github.com:someone/project.git\nCloning creates a new local directory containing the full project history.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Right Tools for Data Science Projects</span>"
    ]
  },
  {
    "objectID": "03-tools.html#first-data-science-project-putting-it-all-together",
    "href": "03-tools.html#first-data-science-project-putting-it-all-together",
    "title": "3  Right Tools for Data Science Projects",
    "section": "3.9 first-data-science-project: Putting It All Together",
    "text": "3.9 first-data-science-project: Putting It All Together\nThis project section is a guided recap. You will reuse the same tools from the earlier gates, in the same order.\nThis chapter integrates everything from Part I—command line, VSCodium, Git, GitHub, Quarto, and Python/R—into a single, coherent project workflow. The goal is to let students experience how real data science work is done: create a clean folder, version it with Git, write a reproducible Quarto file, and share the final analysis.\n\n3.9.1 Choosing a Simple, Meaningful Dataset\nThe first step in any project is choosing a dataset that is small, clean, and intrinsically interesting. Students should select something they care about so they remain motivated while practicing the workflow. Good examples include:\n\nNYC 311 complaint counts for a single neighborhood\nSchool lunch nutrition data from USDA open data\nA small sports dataset (NBA scores, soccer goals, WNBA box scores)\nTrends in daily steps from a personal fitness tracker\nAny two-column CSV they record themselves (date + measurement)\n\nBest practice is to avoid large, messy datasets for this first project. Students should aim to complete end-to-end analysis, not get stuck in heavy cleaning.\n\n\n3.9.2 Setting Up the Project Folder\nA clean folder structure helps keep the project reproducible and organized. Students use the command line to create folders and set up a Git repository.\nRecommended structure:\n\ndata/ — raw datasets in CSV or JSON\nanalysis/ — Quarto notebooks\nfigures/ — automatically generated plots\nREADME.md — short description of the project\n\nKey steps:\n\nUse the command line to create the folder and subfolders\n(mkdir my-project, cd my-project, mkdir data analysis figures)\nInitialize Git with\ngit init\nMake the first commit with\ngit add README.md and git commit -m \"Initial project structure\"\n\nStudents should verify that Git is tracking the project by running git status and confirming the working tree is clean.\n\n\n3.9.3 Writing a Full Quarto Analysis\nThe core of the project is a reproducible Quarto notebook that explains the data, code, and conclusions in one document. The notebook should include:\n\nA clear statement of the question (e.g., “How do 311 noise complaints differ between weekdays and weekends?”)\nCode to import the dataset\nTwo or three meaningful visualizations (bar plots, line plots, scatterplots, histograms)\nShort summary paragraphs explaining the patterns\n\nA minimal workflow:\n\nCreate analysis/project.qmd in VSCodium.\nAdd a YAML header with a title, author, and format.\nInsert code chunks to load the dataset and inspect its structure.\nGenerate plots and save outputs to the figures/ folder.\nRender the notebook to HTML using the VS Codium Quarto extension.\n\nStudents should keep text and code in one place—not separate PowerPoints, Word files, or screenshots. Quarto ensures everything is reproducible.\n\n\n3.9.4 Publishing or Sharing Work\nOnce the analysis renders cleanly, students can make the project public (or share privately).\nOptions include:\n\nPush the project to GitHub with\ngit add README.md analysis/project.qmd, git commit, and git push\nShare the rendered HTML via a GitHub repository\nOptionally, enable GitHub Pages so the report becomes a public website at https://username.github.io/my-project\n\nThis final step completes the full data science cycle: version control, reproducible notebook, and public sharing.",
    "crumbs": [
      "Part 1: Preparing the Tools",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Right Tools for Data Science Projects</span>"
    ]
  }
]